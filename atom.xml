<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Song&#39;s blog</title>
  <subtitle>Easy words so hard to say</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shesong.org/"/>
  <updated>2017-02-25T08:20:18.682Z</updated>
  <id>http://www.shesong.org/</id>
  
  <author>
    <name>She Song</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>社区发现综述简介</title>
    <link href="http://www.shesong.org/2017/02/22/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E7%BB%BC%E8%BF%B0%E7%AE%80%E4%BB%8B/"/>
    <id>http://www.shesong.org/2017/02/22/社区发现综述简介/</id>
    <published>2017-02-22T03:10:08.000Z</published>
    <updated>2017-02-25T08:20:18.682Z</updated>
    
    <content type="html"><![CDATA[<h2 id="研究现状">1 研究现状</h2>
<p>目前流行的网络聚类方法是社区发现，已有的社区发现方法主要用来发现类内链接紧密、类间链接稀疏的网络结构(以下称该类结构为“<strong>传统社区</strong>”)。社区结构对深入理解网络拓扑结构、挖掘网络潜在模式、预测网络行为都具有十分重要的意义。而实际网络的结构比较复杂，人们预先并不知道网络中存在什么结构。</p>
<p>传统社区用于发现网络中<strong>紧密链接的聚类结构</strong>，根据节点链接紧密特性定义聚类中节点的相似性。当网络中不存在社区结构或存在其它结构时，传统社区发现方法不能有效识别网络的真实结构。</p>
<p>近来研究者提出采用一些<strong>概率生成模型</strong>，可发现网络中传统社区及之外的更多类型的聚类结构，该类聚类方法假设同类节点与它类具有相同的链接概率。基于该假设可刻画多种类型网络结构，如</p>
<ul>
<li><em>同类节点链接紧密、与异类节点链接稀疏的结构</em>(传统社区)；</li>
<li><em>与同类节点链接稀疏、与异类节点链接紧密的结构</em>(二分图或多分图)；</li>
<li><em>星型链接模式结构</em>。</li>
</ul>
<p>因此，该类方法可发现比传统社区更广义的聚类结构，以下统称为“<strong>广义社区</strong>”结构，发现这种结构的方法为广义社区发现方法。</p>
<p>传统社区发现的问题：</p>
<ul>
<li>传统社区发现识别的聚类结构不准确。许多流行的传统社区发现方法基于模块度函数求解社区结构，存在分辨率和尺度问题，如社区发现结果易淹没小的社区。因此，该类方法的聚类结果不能反映实际网络的聚类事实。</li>
<li>传统社区发现在网络潜在结构不存在紧密链接子图结构时可能失效，也不能发现网络潜在的其它类型结构。在线社交平台的网络结构规律复杂，我们很难获取关于网络的先验知识，不知道网络是传统社区结构、二分图结构或其它多种类型结构的混合。大多传统社区发现方法假设网络中存在链接紧密的子图结构，且只能发现此类结构。</li>
<li>传统社区发现不能在发现网络社区结构的同时，识别出类间的交互规律：传统社区发现方法的目的是将网络节点按照链接紧密性聚类，不能提供社区间链接模式，不易于网络结构可视化及直观了解网络交互规律。</li>
</ul>
<hr>
<h2 id="复杂网络上的社区发现">2 复杂网络上的社区发现</h2>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/24/58af8e6a537f0.png">

</div>
<table>
<colgroup>
<col width="15%">
<col width="31%">
<col width="18%">
<col width="17%">
<col width="15%">
</colgroup>
<thead>
<tr class="header">
<th>算法</th>
<th>优化目标</th>
<th>计算复杂度</th>
<th>适用情况</th>
<th>局限</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q modularity</td>
<td>最大化Q-modularity</td>
<td><span class="math inline">\(\mid V\mid^2\)</span></td>
<td>无向无权多分量</td>
<td>不适合小网络</td>
</tr>
<tr class="even">
<td>Edge-Betweenness</td>
<td>最小化社区间连边的betweenness</td>
<td><span class="math inline">\(\mid V\mid\mid E\mid^2\)</span></td>
<td>有向有权多分量</td>
<td>慢</td>
</tr>
<tr class="odd">
<td>Leading Eigenvector</td>
<td>对拉普拉斯矩阵第二小特征根对应的特征向量聚类</td>
<td><span class="math inline">\(\mid V\mid^2+\mid E\mid\)</span></td>
<td>无向无权多分量</td>
<td></td>
</tr>
<tr class="even">
<td>Fast Greedy</td>
<td>使用社区合并算法来快速搜索最大Q-modularity</td>
<td><span class="math inline">\(\mid E\mid\log\mid V\mid\)</span></td>
<td>无向有权多分量</td>
<td>不适合小网络</td>
</tr>
<tr class="odd">
<td>Multi Level</td>
<td>使用社区展开算法来快速搜索最大Q-modularity</td>
<td><span class="math inline">\(\mid V\mid\)</span></td>
<td>无向有权多分量</td>
<td>不适合小网络</td>
</tr>
<tr class="even">
<td>Walk Trap</td>
<td>最大化社区间的流距离</td>
<td><span class="math inline">\(\mid E\mid\mid V\mid^2\)</span></td>
<td>无向有权多分量</td>
<td></td>
</tr>
<tr class="odd">
<td>Label Propagation</td>
<td>每个节点取邻居中最流行的标签，迭代式收敛</td>
<td><span class="math inline">\(\mid V\mid+\mid E\mid\)</span></td>
<td>无向有权多分量</td>
<td>结果不稳定</td>
</tr>
<tr class="even">
<td>Info map</td>
<td>最小化随机流的编码长度</td>
<td><span class="math inline">\(\mid V\mid(\mid V\mid+\mid E\mid)\)</span></td>
<td>有向有权单分量</td>
<td></td>
</tr>
<tr class="odd">
<td>Role-based community</td>
<td>划分出在流中地位类似的节点</td>
<td><span class="math inline">\(\mid V\mid^3\)</span></td>
<td>有向有权单分量</td>
<td>结果不稳定</td>
</tr>
</tbody>
</table>
<h3 id="传统算法-traditional-methods">2.1 传统算法 (Traditional methods)</h3>
<h4 id="图分割-graph-partitioning">2.1.1 图分割 (Graph partitioning)</h4>
<p>社区可以看做密集子图结构，使用图分割算法来解决。图分割问题的目标是把图中的节点分成gg个预定大小的群组，这些群组之间的边数目最小，这个问题是NP-hard 的。</p>
<ul>
<li><strong>K-L算法（Kernighan-Lin algorithm）</strong>通过基于贪婪优化的启发式过程把网络分解为2个规模已知的社区。该算法为网络的划分引入一个增益函数，定义为两个社区内部的边数与两个社区边数之间的差，寻求Q的最大划分办法。[1]</li>
<li><strong>谱二分法（spectral bisection method）</strong>早期的分割都是二分图，社区发现也是基于二分的，遇到多分的情况就把其中一个子图再分割。比较经典的有谱二分法，利用拉普拉斯矩阵的第二小特征值<span class="math inline">\(\lambda_2\)</span>对社区二分类，这其实是属于谱方法的一种特例。[2]</li>
<li><strong>最大流（maximum flows）</strong>基于最大流的算法是G.W.Flake提出的。他给网络加了虚拟源节点ss和终点节点tt，并证明了经过最大流算法之后，包含源点ss的社区恰好满足社区内节点链接比与社区外的链接要多的性质。[3] [4]</li>
<li><strong>多层次图分割（level-structure partitioning）</strong>[5]</li>
</ul>
<h4 id="聚类-clustering">2.1.2 聚类 (Clustering)</h4>
<p>当社区的边非常密集，数目远大于点时，图分割可能就不太好使了，这时候社区发现可能更接近于聚类。我们把社区发现看做一组内容相似的物体集合，使用聚类算法。和图中的社区发现相比，图中的社区点与点之间可以用边来表示联系的紧密，而聚类中的社区，需要定义点之间的相似度，比如说根据邻接关系定义：<span class="math display">\[d_{ij}=\sqrt{\sum_{k\neq i,j}(a_{ik}-a_{jk})^2}\]</span>其中<span class="math inline">\(\mathbf{A}=(a_{ij})\)</span>为邻接矩阵，<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>的邻居越多，节点相似度越高。 聚类算法和网络发现（聚类相关的）算法可以很容易地互相转化。另外，社区发现可以是局部的，而聚类是全网络的。</p>
<ul>
<li><strong>层次聚类（hierarchical clustering）</strong>层次聚类假设社区是存在层次结构的（其实不一定额，可能是中心结构），计算网络中每一对节点的相似度。 然后分为凝聚法和分裂法两种：[6]
<ul>
<li><em>凝聚法（Agglomerative algorithms）</em>：根据相似度从强到弱连接相应节点对，形成树状图（Dendrogram），根据需求对树状图进行横切，获得社区结构。</li>
<li><em>分裂法（Divisive algorithms）</em>：找出相互关联最弱的节点，并删除他们之间的边，通过这样的反复操作将网络划分为越来越小的组件，连通的网络构成社区。</li>
</ul></li>
<li><strong>划分聚类（Partitional clustering）</strong>像k-means就很好，可以使用上面的相似度来聚类 。[7]</li>
<li><strong>谱聚类（Spectral clustering）</strong>图分割中的如 Ratio Cut和Normalized Cut其实和谱聚类是等价的，所以谱聚类也能用在社区发现上。</li>
<li><strong>局部聚类</strong></li>
</ul>
<h3 id="分裂方法divisive-algorithms">2.2 分裂方法（Divisive algorithms）</h3>
<p>这里的分裂法和层次聚类中的类似，区别是前者不计算节点相似度，而是删除是两个社区之间的关联边，这些边上的两点的相似度不一定很低。其中最著名的算法就是<strong>Girvan-Newman算法</strong>，根据以下假设：社区之间所存在的少数几个连接应该是社区间通信的瓶颈，是社区间通信时通信流量的必经之路。如果我们考虑网络中某种形式的通信并且寻找到具有最高通信流量（比如最小路径条数）的边，该边就应该是连接不同社区的通道。Girvan-Newman算法就是这样，迭代删除<strong>边介数（Edge Betweenness）</strong>最大的边。[8]</p>
<h3 id="基于模块度的方法modularity-based-methods">2.3 基于模块度的方法（Modularity-based methods）</h3>
<blockquote>
<p>Newman在自己2006年的文章里也证明了Modularity其实是对模块度矩阵进行一个谱分析。这也体现了Modularity方法的普适性和良好的可解释性。</p>
</blockquote>
<p>模块度[9]不仅仅作为优化的目标函数提出，它也是目前是最流行的用来衡量社区结果好坏的标准之一（它的提出被称作社区发现研究历史上的里程碑)。我们知道，社区是节点有意识地紧密联系所造成的，它内部边的紧密程度总比一个随机的网络图来的紧密一些，模块度的定义就是基于此，它表示所有被划分到同一个社区的边所占的比例，再减除掉完全随机情况时被划分到同一个社区的边所占的比例： <span class="math display">\[ \begin{align}
Q=\frac{1}{2M}\sum_{ij}(a_{i,j}-\frac{k_ik_j}{2M})\delta(C_i,C_j)=\frac{1}{2M}\sum_{ij}b_{ij}\delta(C_i,C_j)
\end{align} \]</span> 其中<span class="math inline">\(\mathbf{A}=(a_{ij})\)</span>是网络的邻接矩阵，<span class="math inline">\(M\)</span>是整个图中边的数目，<span class="math inline">\(k_i\)</span> 是顶点<span class="math inline">\(i\)</span>的度数，<span class="math inline">\(C_i\)</span>表示节点<span class="math inline">\(i\)</span>所属的社区，如果两个节点属于同一个社区，则<span class="math inline">\(\delta\)</span>取值1，否则为0。注意公式（1） 的意义，<span class="math inline">\(\frac{a_{ij}}{2M}\)</span>是两个顶点之间连接的概率，如果我们保持一个网络的度分布但对其连边进行随机洗牌，任意一对节点在洗牌后存在连接的概率为<span class="math inline">\(\frac{k_ik_j}{(2M)^2}\)</span>。上式中括号表达的就是节点之间的实际连边概率高于期待值的程度。另<span class="math inline">\(\mathbf{B}=(b_{ij})_{N\times N}\)</span>也称为<strong>模块度矩阵</strong>（Modularity matrix）。</p>
<p>公式还可以写成另一种形式： <span class="math display">\[ \begin{align}
Q=\sum_{i=1}^{n_c}(e_{ii}-a_i^2)
\end{align} \]</span> 其中 <span class="math inline">\(e_{ij}=\sum\limits_{vw}\frac{a_{vw}}{2M}\delta(C_i, C_j)\)</span>，且 <span class="math inline">\(e_{ii}\)</span> 是每个社团内区<span class="math inline">\(i\)</span>内部顶点之间的连边数占整个网络边数的比例，<span class="math inline">\(a_i=\frac{k_i}{2M}=\sum_je_{ij}\)</span>意味着一端与社团 <span class="math inline">\(i\)</span> 中顶点相连的连边的比例，<span class="math inline">\(n_c\)</span> 是整个网络中社区的数量。</p>
<p>在实际计算里，上式要求对社区及其内部节点进行遍历，这个计算复杂度是很大的。Newman(2006) [16]对上式进行了化简，得到矩阵表达如下： <span class="math display">\[ \begin{align}
Q=\frac{1}{2M}\sum_{ij}\sum_r\left[a_{ij}-\frac{k_ik_j}{2m}\right]S_{ir}S_{jr}=\frac{1}{2M}{\rm Tr}(\mathbf{S}^{\rm T}\mathbf{B}\mathbf{S})
\end{align} \]</span> 其中<span class="math inline">\(S\)</span>是<span class="math inline">\(N\times n_c\)</span>的矩阵，且如果顶点<span class="math inline">\(i\)</span>属于社区<span class="math inline">\(r\)</span>则<span class="math inline">\(S_{ir}\)</span>是1，否则为0.</p>
<p>模块度的一个优点是好坏与社区中点的数目无关。模块度真是个好东西，第一次对社区这个模糊的概念提出了量化的衡量标准（不过据说对于小粒度的不太准）。所以对模块度的算法优化多种多样，从贪心到模拟退火等应有尽有。</p>
<ul>
<li><strong>贪心策略（Greedy techniques）</strong> 有两个比较经典的算法，分别是
<ul>
<li>Fast greedy算法（<em>CNN算法</em>）[13]</li>
<li>Multilevel算法（<em>Fast-Unfolding算法</em>）[14]：Fast-Unfolding算法可以Spark中实现。[15]</li>
</ul></li>
<li><strong>模拟退火（Simulated annealing）</strong></li>
<li><strong>极值优化（Extremal optimization）</strong></li>
<li><strong>谱优化（Spectral optimization）</strong></li>
</ul>
<h3 id="谱方法spectral-algorithms">2.2.4 谱方法（Spectral Algorithms）</h3>
<p>基于谱分析的社区算法基于如下事实，在同一个社区内的节点，它在拉普拉斯矩阵中的特征向量近似。将节点对应的矩阵特征向量（与特征值和特征向量有关的都叫谱）看成空间坐标，将网络节点映射到多维向量空间去，然后就可以运用传统的聚类算法将它们聚集成社团。这种方法不可避免的要计算矩阵的特征值，开销很大，但是因为能直接使用很多传统的向量聚类的成果，灵活性很高。</p>
<h3 id="动态算法dynamic-algorithms">2.5 动态算法（Dynamic Algorithms）</h3>
<p>自旋模型和同步算法应该是物理学家提出来的算法，话说物理学家在社区发现领域十分活跃，发了不少论文。随机游走是基于以下思想：如果存在很强的社区结构，那么随机游走器（random walker)会在社区内部停留更长的时间，因为社区内部的边密度比较高。</p>
<ul>
<li><strong>自旋模型（Spin models）</strong></li>
<li><strong>随机游走（Random walk）</strong></li>
<li><strong>同步算法（Synchronization）</strong></li>
</ul>
<h3 id="基于统计推断的算法methods-based-on-statistical-inference">2.6 基于统计推断的算法（Methods based on statistical inference）</h3>
<p>基于统计推断的方法包括观察到的数据集和对模型的假设。如果数据集是图，模型假设对节点之间如何联系的描述就要符合真实的图结构。[10] [11]</p>
<ul>
<li><strong>生成模型（Generative models）</strong></li>
<li><strong>判别模型（Blockmodeling, model selection and information theory）</strong></li>
</ul>
<blockquote>
<p>赫赫有名的<strong>infomap</strong>这个方法是从编码的角度给了社区一个良好的解释，发现了网络的一种最优的二级编码就发现了对应的社区结构。思想是相当绝妙的，效果也很好。[12]</p>
</blockquote>
<hr>
<h2 id="社区发现算法特征">3 社区发现算法特征</h2>
<h3 id="优化目标">3.1 优化目标</h3>
<p>有一些社区发现算法比如谱方法，K-L算法，以及基于最大流的社区发现方法等，给出明确的的目标函数，并提出算法来最优化目标函数。 常用的优化目标函数有：</p>
<h4 id="normailized-cutratio-cut和conductance">3.1.1 Normailized Cut、Ratio Cut和conductance</h4>
<p>记<span class="math inline">\(\pi=(C_1,C_2,\cdots,C_K)\)</span>表示网络的社区，同时满足<span class="math inline">\(C_i\bigcap C_j=\emptyset,\, \bigcup_{i=1}^KC_i=V\)</span>，<strong>规范化割集标准（Normailized Cut</strong>）和<strong>比例割集标准（Ratio Cut）</strong>定义为： <span class="math display">\[ \begin{align}
Ratio\, Cut(\pi)=\frac{1}{K}\sum_{i=1}^K\frac{cut(C_i,\overline{C_i})}{|C_i|}
\end{align} \]</span> <span class="math display">\[ \begin{align}
Normailized\, Cut(\pi)=\frac{1}{K}\sum_{i=1}^K\frac{cut(C_i,\overline{C_i})}{vol(C_i)}, vol(C_i)=\sum_{v\in C_i}k_v
\end{align} \]</span></p>
<p>其中 <span class="math inline">\(cut(C_i,\overline{C_i})\)</span> 表示两者之间的联系数目。两个目标函数的目标是使得社区之间的边数量最小化的同时，避免出现类似于只包含1个成员的小社区。</p>
<p>连通度(conductance)也是类似的定义: <span class="math display">\[ \begin{align}
Conductance(C_i)=\frac{cut(C_i,\overline{C_i})}{\min\{vol(C_i),vol(\overline{C_i})\}}
\end{align} \]</span></p>
<h4 id="kernighan-lin-object">3.1.2 Kernighan-Lin object</h4>
<p>K-L目标函数旨在使两个相同大小的社区之间的边联系最小： <span class="math display">\[ \begin{align}
KL_{obj}(C_1,\ldots,C_k)=\sum_{i\neq j}A(C_i,C_j)
\end{align} \]</span> 其中<span class="math inline">\(A(C_i,C_j)=\sum_{u\in C_i,v\in C_j}A(u,v), |C_1|=|C_2|=\ldots=|C_k|\)</span></p>
<h4 id="modularity">3.1.3 Modularity</h4>
<p>模块度已在2.3节提过。</p>
<h3 id="粒度控制社区数目可不可控">3.2 粒度控制（社区数目可不可控）</h3>
<p>对于有层次的社区发现算法来说的，比如某些二分社区算法，是通过不断递归的划分子社区来获得预定的社区数目。而某些算法，像层次聚类和MCL（Markov clustering），基于概率模型的社区发现算法等，允许用户通过调节参数来间接控制输出社区的数目。</p>
<p>另一些算法，像模块度优化算法，它的社区数目是由优化函数决定的，不需要用户来设定社区的数目。</p>
<h3 id="规模">3.3 规模</h3>
<p>很多算法在设计的时候，并没有特别地考虑伸缩性，在面对整个Web以及大型社交网络时动辄百万甚至千万个点时效果不佳。比如G-N算法，需要计算即通过每条边的最短路径数目（edge betweeness)，复杂度相当高，像谱聚类算法，能处理10K个点和70M条边就不错了。</p>
<p>所以，有些算法比如Shingling算法等，使用的方法相对简单，从而能适合大规模的社区发现的运行要求。</p>
<h3 id="重叠社区">3.4 重叠社区</h3>
<p>很多社区发现算法，比如图分割算法，将整个网络划分为多个独立的社区结构。但是在现实中，许多网络并不存在绝对的彼此独立的社团结构，相反，它们是由许多彼此重叠互相关联的社团构成，比如说在社交网络中，一个人根据兴趣的不同，有可能属于多个不同的小组等。所以，很多类似派系过滤算法（CPM - Clique Percolation Method）[17]这样旨在发现重叠社区的算法也被不断地提出来。</p>
<h3 id="评价标准">3.5 评价标准</h3>
<h4 id="准确率召回率f1值">3.5.1 准确率，召回率，F1值</h4>
<p>一个大规模数据集合中检索文档的时，可把文档分成四组：系统检索到的相关文档（A），系统检索到的不相关文档（B），相关但是系统没有检索到的文档（C），不相关且没有被系统检索到的文档（D）： 准确度定义为： <span class="math display">\[ \begin{align}
pr=\frac{A}{A+C}
\end{align} \]</span></p>
<p>召回率定义为： <span class="math display">\[ \begin{align}
rc=\frac{A}{A+B}
\end{align} \]</span></p>
<p>F-measure是准确率和召回率协调之后的结果，定义为： <span class="math display">\[ \begin{align}
PWF=\frac{2\times pr \times rc}{pr+rc}
\end{align} \]</span></p>
<p>同理，社区也可以用这个概念。</p>
<h4 id="平均聚类纯度average-cluster-purity">3.5.2 平均聚类纯度（average cluster purity）</h4>
<p>假设算法发现了<span class="math inline">\(C=\{C_1,\ldots,C_K\}\)</span>个社区，我们假设社区<span class="math inline">\(C_i\)</span>有<span class="math inline">\(n_i\)</span>个点，，每个点分别为<span class="math inline">\(\{v_{1,i},\ldots,v_{n_i,i}\}\)</span>。令<span class="math inline">\(M_{l,i}\)</span>为<span class="math inline">\(v_{l,i}\)</span>真实归属的社区的标签，平均聚类纯度为定义为： <span class="math display">\[ \begin{align}
ACP=\frac{1}{k}\sum_{i=1}^k\sum_{l=1}^{n_i}\frac{\delta(dom_i\in M_{l,i})}{n_i}
\end{align} \]</span> 即社区<span class="math inline">\(C_i\)</span>中主要标签的点占社区所有点的数目比例。</p>
<h4 id="互信息">3.5.3 互信息</h4>
<p>首先来回顾熵的定义,在一个分布内包含的信息为熵： <span class="math display">\[ \begin{align}
H(X)=-\sum_{x \in X}p(x)\log p(x)
\end{align} \]</span></p>
<p><strong>互信息（mutual information)</strong>描述了两个分布之间的相关性： <span class="math display">\[ \begin{align}
I(X;Y)=H(X)-H(X|Y)=\sum_{y \in Y}\sum_{x \in X}p(x,y)\log(\frac{p(x,y)}{p(x)p(y)})
\end{align} \]</span></p>
<p>但事实上，对于一个划分<span class="math inline">\(X\)</span>，任何一个从<span class="math inline">\(X\)</span>派生的划分<span class="math inline">\(Y\)</span>都和<span class="math inline">\(X\)</span>有相同的互信息，尽管这些划分都不尽相同。在这种情况下条件熵<span class="math inline">\(H(X|Y)\)</span>都近乎为0，这也导致了互信息<span class="math inline">\(I(X;Y)\)</span>几乎等于<span class="math inline">\(H(X)\)</span>。为了避免这种情况，Danon et al.提出了归一化互信息（normalized mutual information），直到现在都使用得非常广泛：[19] <span class="math display">\[ \begin{align}
I_{norm}(\mathcal{X},\mathcal{Y})=\frac{2I(X;Y)}{H(X)+H(Y)}
\end{align} \]</span></p>
<p>所谓两个事件相关性的量化度量，就是在了解其中一个<span class="math inline">\(Y\)</span>的前提下，对消除另一个<span class="math inline">\(X\)</span>不确定性所提供的信息量。 规范化的互信息定义为 <span class="math display">\[ \begin{align}
NMI(X;Y)=\frac{I(X;Y)}{\sqrt{H(X)H(Y)}}
\end{align} \]</span></p>
<p>我们将划分当做一个结点落在社区的概率分布，然后计算社区划分结果和真实情况的NMI值，具体例子见参考《社会计算:社区发现和社会媒体挖掘》[18].</p>
<hr>
<h2 id="参考文献">参考文献</h2>
<ul>
<li>[1]: <a href="http://ieeexplore.ieee.org/abstract/document/6771089/" target="_blank" rel="external">Kernighan, Brian W., and Shen Lin. “An efficient heuristic procedure for partitioning graphs.” The Bell system technical journal 49.2 (1970): 291-307.</a></li>
<li>[2]: <a href="http://epubs.siam.org/doi/abs/10.1137/0603056?journalCode=sjamdu" target="_blank" rel="external">Barnes, Earl R. “An algorithm for partitioning the nodes of a graph.” SIAM Journal on Algebraic Discrete Methods 3.4 (1982): 541-550.</a></li>
<li>[3]: <a href="http://dl.acm.org/citation.cfm?id=347121" target="_blank" rel="external">Flake, Gary William, Steve Lawrence, and C. Lee Giles. “Efficient identification of web communities.” Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2000.</a></li>
<li>[4]: <a href="http://ieeexplore.ieee.org/abstract/document/989932/" target="_blank" rel="external">Flake, Gary William, et al. “Self-organization and identification of web communities.” Computer 35.3 (2002): 66-70.</a></li>
<li>[5]: <a href="http://link.springer.com/chapter/10.1007/978-94-011-5412-3_12" target="_blank" rel="external">Pothen, Alex. “Graph partitioning algorithms with applications to scientific computing.” Parallel Numerical Algorithms. Springer Netherlands, 1997. 323-368.</a></li>
<li>[6]: <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf" target="_blank" rel="external">Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.</a></li>
<li>[7]: <a href="https://projecteuclid.org/euclid.bsmsp/1200512992" target="_blank" rel="external">MacQueen, James. “Some methods for classification and analysis of multivariate observations.” Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. No. 14. 1967.</a></li>
<li>[8]: <a href="http://www.pnas.org/content/99/12/7821.short" target="_blank" rel="external">Girvan, Michelle, and Mark EJ Newman. “Community structure in social and biological networks.” Proceedings of the national academy of sciences 99.12 (2002): 7821-7826.</a></li>
<li>[9]: <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.69.026113" target="_blank" rel="external">Newman, Mark EJ, and Michelle Girvan. “Finding and evaluating community structure in networks.” Physical review E 69.2 (2004): 026113.</a></li>
<li>[10]: <a href="http://www.cnki.com.cn/Article/CJFDTotal-JSJA201208000.htm" target="_blank" rel="external">柴变芳, 贾彩燕, and 于剑. “基于统计推理的社区发现模型综述.” 计算机科学 2012 年 08 (2012): 1-7+.</a></li>
<li>[11]: <a href="http://cdmd.cnki.com.cn/Article/CDMD-10004-1015611891.htm" target="_blank" rel="external">柴变芳. 基于生成模型的大规模网络广义社区发现方法研究. Diss. 北京交通大学, 2015.</a></li>
<li>[12]: <a href="http://www.mapequation.org/assets/publications/RosvallBergstromPNAS2008Full.pdf" target="_blank" rel="external">Rosvall, Martin, and Carl T. Bergstrom. “Maps of random walks on complex networks reveal community structure.” Proceedings of the National Academy of Sciences 105.4 (2008): 1118-1123.</a></li>
<li>[13]: <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.70.066111" target="_blank" rel="external">Clauset, Aaron, Mark EJ Newman, and Cristopher Moore. “Finding community structure in very large networks.” Physical review E 70.6 (2004): 066111.</a></li>
<li>[14]: <a href="https://arxiv.org/abs/0803.0476v2" target="_blank" rel="external">Blondel, Vincent D., et al. “Fast unfolding of communities in large networks.” Journal of statistical mechanics: theory and experiment 2008.10 (2008): P10008.</a></li>
<li>[15]: <a href="http://bbs.pinggu.org/thread-3614747-1-1.html" target="_blank" rel="external">基于GraphX的社区发现算法FastUnfolding分布式实现</a></li>
<li>[16]: <a href="http://www.pnas.org/content/103/23/8577.short" target="_blank" rel="external">Newman, Mark EJ. “Modularity and community structure in networks.” Proceedings of the national academy of sciences 103.23 (2006): 8577-8582.</a></li>
<li>[17]: <a href="http://www.nature.com/nature/journal/v435/n7043/abs/nature03607.html" target="_blank" rel="external">Palla, Gergely, et al. “Uncovering the overlapping community structure of complex networks in nature and society.” Nature 435.7043 (2005): 814-818.</a></li>
<li>[18]: <a href="https://github.com/rexwong/dp/wiki/%E7%A4%BE%E4%BC%9A%E8%AE%A1%E7%AE%97-%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E6%8C%96%E6%8E%98" target="_blank" rel="external">LeiTang, and HuanLin. 社会计算:社区发现和社会媒体挖掘. 机械工业出版社, 2013.</a></li>
<li>[19]: <a href="http://iopscience.iop.org/article/10.1088/1742-5468/2005/09/P09008/meta" target="_blank" rel="external">Danon, Leon, et al. “Comparing community structure identification.” Journal of Statistical Mechanics: Theory and Experiment 2005.09 (2005): P09008.</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;研究现状&quot;&gt;1 研究现状&lt;/h2&gt;
&lt;p&gt;目前流行的网络聚类方法是社区发现，已有的社区发现方法主要用来发现类内链接紧密、类间链接稀疏的网络结构(以下称该类结构为“&lt;strong&gt;传统社区&lt;/strong&gt;”)。社区结构对深入理解网络拓扑结构、挖掘网络潜在模式、预
    
    </summary>
    
      <category term="Community Detection" scheme="http://www.shesong.org/categories/Community-Detection/"/>
    
    
      <category term="调研" scheme="http://www.shesong.org/tags/%E8%B0%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>A MapReduce Algorithm for Matrix-Vector Multiplication</title>
    <link href="http://www.shesong.org/2016/10/17/A-MapReduce-Algorithm-for-Matrix-Vector-Multiplication/"/>
    <id>http://www.shesong.org/2016/10/17/A-MapReduce-Algorithm-for-Matrix-Vector-Multiplication/</id>
    <published>2016-10-17T01:35:24.000Z</published>
    <updated>2017-02-21T11:31:28.869Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题定义">问题定义</h3>
<p>假设有矩阵 <span class="math inline">\(\mathcal{A}=(a_{i,k})\in\mathbb{R}^{I\times K}\)</span>，其中 <span class="math inline">\(i\in[0,I),k\in[0,K)\)</span>，向量 <span class="math inline">\(\mathcal{B}=(b_{k,0})\in\mathbb{R}^{K\times 1}\)</span>，其中 <span class="math inline">\(k\in[0,K)\)</span>。我们作矩阵乘向量运算，可得向量 <span class="math inline">\(\mathcal{C}=\mathcal{A}\mathcal{B}=(c_{i,0})\in\mathbb{R}^{I\times 1}\)</span>，其中 <span class="math inline">\(c_{i,0}=\sum\limits_{k=0}^{K-1}a_{i,k}b_{k,0}\)</span>。当矩阵大到一定程度时，一台服务器由于内存限制已经无法处理。在此我们考虑基于分块的大矩阵乘法，并在Hadoop平台上实现该算法。</p>
<p>我们将 <span class="math inline">\(\mathcal{A}\)</span> 和 <span class="math inline">\(\mathcal{B}\)</span> 划分为足够小的块（子矩阵），使得 <span class="math inline">\(\mathcal{A}\)</span> 和 <span class="math inline">\(\mathcal{B}\)</span> 的小块可以在集群中的单个节点上的内存中相乘。设 <span class="math inline">\(IB\)</span> 表示分块后每个 <span class="math inline">\(\mathcal{A}\)</span> 块和 <span class="math inline">\(\mathcal{C}\)</span> 块的行数，<span class="math inline">\(KB\)</span> 表示每个 <span class="math inline">\(\mathcal{A}\)</span> 块的列数和 <span class="math inline">\(\mathcal{B}\)</span> 块的行数，<span class="math inline">\(NIB\)</span> 表示 <span class="math inline">\(\mathcal{A}\)</span> 行和 <span class="math inline">\(\mathcal{C}\)</span> 行的划分数，即 <span class="math inline">\(NIB=\frac{I-1}{IB}+1\)</span>，<span class="math inline">\(NKB\)</span> 表示 <span class="math inline">\(\mathcal{A}\)</span> 列和 <span class="math inline">\(\mathcal{B}\)</span> 行的划分数，即 <span class="math inline">\(NKB=\frac{K-1}{KB}+1\)</span>。</p>
<hr>
<h3 id="示例">示例</h3>
<p>矩阵 <span class="math inline">\(\mathcal{A}=\left[\begin{array}{cc|cc}a_{00} &amp; a_{01} &amp; a_{02} &amp; a_{03}\\a_{10} &amp; a_{11} &amp; a_{12} &amp; a_{13}\\\hline a_{20} &amp; a_{21} &amp; a_{22} &amp; a_{23}\\a_{30} &amp; a_{31} &amp; a_{32} &amp; a_{33}\\\hline a_{40} &amp; a_{41} &amp; a_{42} &amp; a_{43}\\a_{50} &amp; a_{51} &amp; a_{52} &amp; a_{53}\\\end{array}\right]\)</span> 被分成 <span class="math inline">\(3\times 2\)</span> 个块，记为 <span class="math inline">\(\mathcal{A}=\left[\begin{array}{cc}\mathbf{A}_{00} &amp; \mathbf{A}_{01}\\\mathbf{A}_{10} &amp; \mathbf{A}_{11}\\\mathbf{A}_{20} &amp; \mathbf{A}_{21}\\\end{array}\right]\)</span>。</p>
<p>向量 <span class="math inline">\(\mathcal{B}=\left[\begin{array}{c}b_{00}\\b_{10}\\\hline b_{20}\\b_{30}\end{array}\right]\)</span> 被分为 <span class="math inline">\(2\times 1\)</span> 个块，记为 <span class="math inline">\(\mathcal{B}=\left[\begin{array}{c}\mathbf{B}_{00}\\\mathbf{B}_{10}\end{array}\right]\)</span>。</p>
<p>则矩阵 <span class="math inline">\(\mathcal{C}=\mathcal{A}\mathcal{B}=\left[\begin{array}{c}\mathbf{C}_{00}\\\mathbf{C}_{10}\\\mathbf{C}_{20}\end{array}\right]=\left[\begin{array}{c}\mathbf{A}_{00}\mathbf{B}_{00}\\\mathbf{A}_{10}\mathbf{B}_{00}\\\mathbf{A}_{20}\mathbf{B}_{00}\end{array}\right]+\left[\begin{array}{c}\mathbf{A}_{01}\mathbf{B}_{10}\\\mathbf{A}_{11}\mathbf{B}_{10}\\\mathbf{A}_{21}\mathbf{B}_{10}\end{array}\right]=\left[\begin{array}{c}\mathbf{A}_{00}\mathbf{B}_{00}+\mathbf{A}_{01}\mathbf{B}_{10}\\\mathbf{A}_{10}\mathbf{B}_{00}+\mathbf{A}_{11}\mathbf{B}_{10}\\\mathbf{A}_{20}\mathbf{B}_{00}+\mathbf{A}_{21}\mathbf{B}_{10}\end{array}\right]\)</span>，此时 <span class="math inline">\(I=6,K=4,IB=2,KB=2,NIB=3,NKB=2\)</span>。</p>
<p>我们令 <span class="math inline">\(0\leq ib&lt;NIB\)</span> 和 <span class="math inline">\(0\leq kb&lt;NKB\)</span> 表示某个块的下标，如当 <span class="math inline">\(ib=1,\,kb=1\)</span> 时，则 <span class="math inline">\(\mathbf{A}_{ib,kb}=\left[\begin{array}{cc}a_{22} &amp; a_{23}\\a_{32} &amp; a_{33}\end{array}\right]\)</span>，<span class="math inline">\(\mathbf{B}_{kb,0}=\left[\begin{array}{c}b_{20}\\b_{30}\end{array}\right]\)</span>。</p>
<hr>
<h3 id="在mapreduce上实现分块算法">在MapReduce上实现分块算法</h3>
<p>我们只用一个MapRedcue阶段来实现算法。Mappper负责读取数据，并按照分块的策略发送分块，注意的是矩阵和向量分别存储于两个不同的输入路径且同时作为Mapper的输入数据。每个Reducers接收到Mapper发送的分块后，根据分块算法计算对应分块乘积的结果。例如，Reducer接收到块 <span class="math inline">\(\mathbf{A}_{ib,kb}\)</span> 和 <span class="math inline">\(\mathbf{B}_{kb,0}\)</span>，其中 <span class="math inline">\(0\leq kb&lt; NKB\)</span>，并按以下顺序重新组织：<span class="math display">\[\begin{align}\mathbf{A}_{ib,0}\,\,\mathbf{B}_{0,0}\,\,\mathbf{A}_{ib,1}\,\,\mathbf{B}_{1,0}\,\,\cdots \mathbf{A}_{ib,NKB-1}\,\,\mathbf{B}_{NKB-1,0}\end{align}\]</span> Reducer将每两个块 <span class="math inline">\(\mathbf{A}\)</span> 和 <span class="math inline">\(\mathbf{B}\)</span> 相乘并累加结果，即 <span class="math inline">\(\sum\limits_{kb=0}^{KB-1}\mathbf{A}_{ib,kb}\mathbf{B}_{kb,0}\)</span>。</p>
<hr>
<h3 id="算法分析">算法分析</h3>
<p>考虑最糟糕的情形，若矩阵 <span class="math inline">\(\mathcal{A}\)</span> 是稠密的而且无0元素，Mapper需要发送 <span class="math inline">\(I\times K\)</span> 个中间对。同理对于向量 <span class="math inline">\(\mathcal{B}\)</span>，Mapper需要发送 <span class="math inline">\(NIB\times K\)</span> 个中间对。即在最坏的情况下，会有 <span class="math inline">\(K\times (I+NIB)\)</span> 个中间对在soft和shuffle阶段传输。</p>
<hr>
<h3 id="伪代码">伪代码</h3>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/21/58ab9b4b72b77.png">

</div>
<p>注意到Mapper发送的key是个复合key，它由3个元素组成，分别是 <span class="math inline">\((ib,\, kb,\, m)\)</span>，其中 <span class="math inline">\(ib\)</span> 和 <span class="math inline">\(kb\)</span> 分别表示块中元素的行、列下标，而 <span class="math inline">\(m=0\)</span> 表示数据来自矩阵 <span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(m=1\)</span> 来自向量 <span class="math inline">\(\mathcal{B}\)</span>。</p>
<p>在sort和shuffle阶段，复合key必须有序，如按升序则先排 <span class="math inline">\(ib\)</span>，然后 <span class="math inline">\(kb\)</span>，最后 <span class="math inline">\(m\)</span>。同时，Partitioner需按等式 <span class="math inline">\(r=ib\pmod R\)</span> 来将中间对分发到各个Reducer中（<span class="math inline">\(R\)</span> 是Reducer的个数）。以上排序和划分复合key保证了每个Reducer都依据公式（1）接收到对应的 <span class="math inline">\(\mathbf{A}\)</span> 块和 <span class="math inline">\(\mathbf{B}\)</span> 块的数据。</p>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/21/58ab9b4b741c9.png">

</div>
<p>注意在Reducer阶段，我们只需要为矩阵 <span class="math inline">\(\mathcal{A}\)</span> 和向量 <span class="math inline">\(\mathcal{C}\)</span> 开辟内存空间。</p>
<hr>
<h3 id="源码">源码</h3>
<p>相关代码可参考 <a href="https://github.com/francize/Matrix-Vector-Multiplication" class="uri" target="_blank" rel="external">https://github.com/francize/Matrix-Vector-Multiplication</a>。</p>
<hr>
<h3 id="参考">参考</h3>
<ol style="list-style-type: decimal">
<li><a href="http://www.norstad.org/matrix-multiply/" class="uri" target="_blank" rel="external">http://www.norstad.org/matrix-multiply/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Block_matrix" class="uri" target="_blank" rel="external">https://en.wikipedia.org/wiki/Block_matrix</a></li>
<li><a href="https://vangjee.wordpress.com/2012/03/30/implementing-rawcomparator-will-speed-up-your-hadoop-mapreduce-mr-jobs-2/" target="_blank" rel="external">关于实现rawcomparator</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题定义&quot;&gt;问题定义&lt;/h3&gt;
&lt;p&gt;假设有矩阵 &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{A}=(a_{i,k})\in\mathbb{R}^{I\times K}\)&lt;/span&gt;，其中 &lt;span class=&quot;math i
    
    </summary>
    
      <category term="Distributed Computing" scheme="http://www.shesong.org/categories/Distributed-Computing/"/>
    
    
      <category term="MapReduce" scheme="http://www.shesong.org/tags/MapReduce/"/>
    
      <category term="Matrix Multiplication" scheme="http://www.shesong.org/tags/Matrix-Multiplication/"/>
    
  </entry>
  
</feed>
