<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Song&#39;s blog</title>
  <subtitle>Easy words so hard to say</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shesong.org/"/>
  <updated>2017-03-09T06:49:09.806Z</updated>
  <id>http://www.shesong.org/</id>
  
  <author>
    <name>She Song</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[转载]分布式机器学习的故事</title>
    <link href="http://www.shesong.org/2017/03/09/%E8%BD%AC%E8%BD%BD-%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%85%E4%BA%8B/"/>
    <id>http://www.shesong.org/2017/03/09/转载-分布式机器学习的故事/</id>
    <published>2017-03-09T06:37:27.000Z</published>
    <updated>2017-03-09T06:49:09.806Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文转载自 <a href="http://www.flickering.cn/" target="_blank" rel="external">火光摇曳</a></p>
</blockquote>
<h2 id="大数据带来的新机遇">大数据带来的新机遇</h2>
<h3 id="一个新时代">一个新时代</h3>
<h4 id="起源">起源</h4>
<p>分布式机器学习是随着“大数据”概念兴起的。在有大数据之前，有很多研究工作为了让机器学习算法更快，而利多多个处理器。这类工作通常称为“并行计算”或者“并行机器学习”，其核心目标是把计算任务拆解成多个小的任务，分配到多个处理器上做计算。分布式计算或者分布式机器学习除了要把计算任务分布到多个处理器上，更重要的是把数据（包括训练数据以及中间结果）分布开来。因为在大数据时代，一台机器的硬盘往往装不下全部数据，或者即使装下了，也会受限于机器的I/O通道的带宽，以至于访问速度很慢。为了更大的存储容量、吞吐量以及容错能力，我们都希望把数据分布在多台计算机上。</p>
<p>那么什么样的数据大到一台机器甚至几百台机器的硬盘都装不下呢？要知道，现在很多服务器的硬盘空间都是数TB的了！其实这样的大数据有很多。比如搜索引擎要爬下很多很多的网页，对其内容做分析并建立索引。有多少网页呢？这个数字很难估计，因为这是随时间变化的。在Web 2.0出现之前，全球网页数量的增长相对稳定，因为网页都是专业人员编辑的。而由于各种Web 2.0工具帮助用户建立自己的网页，比如博客、甚至微博，所以网页数量呈指数速度递增。</p>
<p>另一种典型的大数据是电商网站上的用户行为数据。比如在亚马逊或者淘宝上，每天都很多用户看到了很多推荐的商品，并且点击了其中一些。这些用户点击推荐商品的行为会被亚马逊和淘宝的服务器记录下来，作为分布式机器学习系统的输入。输出是一个数学模型，可以预测一个用户喜欢看到哪些商品，从而在下一次展示推荐商品的时候，多展示那些用户喜欢的。</p>
<p>类似的，在互联网广告系统中，展示给用户的广告、以及用户点击的广告也都会被记录下来，作为机器学习系统的数据，训练点击率预估模型。在下一次展示推荐商品时，这些模型会被用来预估每个商品如果被展示之后，有多大的概率被用户点击。其中预估点击率高的商品，往往展示在预估点击率低的商品之前，从而赢得实际上比较高的点击率。</p>
<p>从上面的例子我们可以看出来，这些大数据之所以大，是因为它们记录的是数十亿互联网用户的行为。而人们每天都会产生行为，以至于百度、阿里、腾讯、奇虎、搜狗这样的公司的互联网服务每天收集到很多很多块硬盘才能装下的数据。而且这些数据随时间增加，永无止境。虽然对“大数据”的具体定义见人见智，但是互联网用户的行为数据，毫无疑问地被公认为大数据了。</p>
<h4 id="价值">价值</h4>
<p>机器学习的应用由来已久。大家可能还记得十几年前IBM推出的语音识别和输入系统ViaVoice。这个系统使用的声学模型和语言模型是用人工收集整理和标注的数据训练的。当年因为IBM财大气粗，收集和整理了很多数据，所以ViaVoice的识别准确率在同类产品中遥遥领先。但是，ViaVoice很难保证能识别各种口音的人。所以IBM的工程师们设计了一个自动适应的功能——通过让用户标注没能正确识别的语音对应的文本，ViaVoice可以针对主任的口音做特别的优化。</p>
<p>今天，大家可以通过互联网使用Google的语音识别系统。我们会发现，不管使用者口音如何，Google的语音识别系统几乎都能准确识别，以至于几乎不再需要“适应主人的口音”。而且Google的系统支持的语言种类也更多。这其中的奥妙就在于“大数据”。</p>
<p>在Google发布语音识别引擎之前，先有语音搜索服务。在语音搜索服务之前，有一个打电话查询的服务。实际上，正式这个电话服务收集了很多用户的语音输入。这部分数据经过人工标注，称为了训练语言模型和声学模型的第一批数据。随后发布的语音搜索收集了世界各地更多互联网用户的声音，加上半自动标注系统的引入，训练数据大大丰富了。训练数据越多，能覆盖的口音和语种越多，机器学习得到的模型的识别准确率也就越高。以至于当Google发布语音识别引擎之初，识别率就远高于依赖人工标注训练数据的IBM ViaVoice。随着语音识别服务被很多手机应用和桌面应用使用，它能采集更多用户的语音输入，模型的准确性会不断得到提高。</p>
<p>从上面例子我们可以看出，因为互联网服务收集的数据是万万千千用户的行为的体现，而人类行为是人类智能的结果。所以如果我们能设计分布式机器学习系统，能从大数据中归纳规律，我们实际上就在归纳整个人类的知识库。这个听起来很神奇，实际上在上面的例子里，Google已经做到了。在这一系列的最后一节里，我们会介绍我们开发的一个语义学习系统，它从上千亿条文本数据中，归纳汉语中上百万的“语义”。随后，只要用户输入任何一段文本，这个系统可以利用训练好的模型在一毫秒之内，理解文本中表达的“语义”。这个理解过程确保消除文本中的歧义，从而让搜索引擎、广告系统、推荐系统等应用更好地理解用户需求。</p>
<p>简言之，互联网使得人类第一次有机会收集全人类的行为数据。从而为机器学习这一持续了数十年的研究方向提供了全新的机会——分布式机器学习——从互联网数据中归纳这个人类的知识，从而让机器比任何一个个人都要“聪明”。</p>
<h2 id="分布式机器学习的评价标准">分布式机器学习的评价标准</h2>
<p>在后文中会详细介绍的各个大规模机器学习系统，基本都有三个特点：</p>
<ol style="list-style-type: decimal">
<li><strong>可扩展</strong>。可扩展的意思是“投入更多的机器，能处理更大的数据”。而传统的并行计算要的是：“投入更多机器，数据大小不变，计算速度更快”。这是我认识中“大数据”和传统并行计算研究目标不同的地方。如果只是求速度快，那么multicore和GPU会比分布式机器学习的ROI更高。
<ul>
<li>有一个框架（比如MPI或者MapReduce或者自己设计的），支持fault recovery。Fault recovery是可扩展的基础。现代机群系统都是很多用户公用的，其中任何一个进程都有可能被更高优先级的进程preempted。一个job涉及数千个进程（task processes），十分钟里一个进程都不挂的概率很小。而如果一个进程挂了，其他进程都得重启，那么整个计算任务可能永远都不能完成。</li>
</ul></li>
<li><strong>数学模型要根据架构和数据做修改</strong>。这里有两个原因：
<ul>
<li>因为大数据基本都是长尾分布的，而papers里的模型基本都假设数据是指数分布的（想想用SVD做component analysis其实假设了Gaussian distributed，latent Dirichlet allocation假设了multimonial distribution。）。真正能处理大数据的数学模型，都需要能更好的描述长尾数据。否则，模型训练就是忽视长尾，而只关注从“大头”数据部分挖掘“主流”patterns了。</li>
<li>很多机器学习算法（比如MCMC）都不适合并行化。所以往往需要根据模型的特点做一些算法的调整。有时候会是approximation。比如AD-LDA算法是一种并行Gibbs sampling算法，但是只针对LDA模型有效，对其他大部分模型都不收敛，甚至对LDA的很多改进模型也不收敛。</li>
</ul></li>
<li><strong>引入更多机器的首要目的不是提升性能，而是能处理更大的数据</strong>。用更多的机器，处理同样大小的数据，期待speedup提高——这是传统并行计算要解决的问题——是multicore、SMP、MPP、GPU还是Beowolf cluster上得分布式计算不重要。在大数据情况下，困难点在问题规模大，数据量大。此时，引入更多机器，是期待能处理更大数据，总时间消耗可以不变甚至慢一点。分布式计算把数据和计算都分不到多台机器上，在存储、I/O、通信和计算上都要消除瓶颈。</li>
</ol>
<p>上述三个特点，会在实践中要求“一个有价值的算法值得也应该有自己独特的框架”。</p>
<p>在开始说故事之前，再先正名几个概念：Message Passing和MapReduce是两个有名的并行程序编程<strong>范式</strong>（paradigm），也就是说，并行程序应该怎么写都有规范了——只需要在预先提供的<strong>框架</strong>（framework）程序里插入一些代码，就能得到自己的并行程序。Message Passing范式的一个框架叫做MPI。MapReduce范式的框架也叫MapReduce。而MPICH2和Apache Hadoop分别是这MPI和MapReduce两个框架的<strong>实现</strong>（implementations）。另一个本文会涉及的MapReduce实现是我用C++写的<a href="https://code.google.com/archive/p/mapreduce-lite" target="_blank" rel="external">MapReduce Lite</a>。后面还会提到<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.799&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">BSP范式</a>，它的一个著名的实现是<a href="http://kowshik.github.io/JPregel/pregel_paper.pdf" target="_blank" rel="external">Google Pregel</a>。</p>
<p>MPI这个框架很灵活，对程序结构几乎没有太多约束，以至于大家有时把MPI称为一组<strong>接口</strong>（interface）——MPI的I就是interface的意思。</p>
<p>这里，MPICH2和Hadoop都是很大的系统——除了实现框架（允许程序员方便的编程），还实现了资源管理和分配，以及资源调度的功能。这些功能在Google的系统里是分布式操作系统负责的，而Google MapReduce和Pregel都是在分布式操作系统基础上开发的，框架本身的代码量少很多，并且逻辑清晰易于维护。当然Hadoop已经意识到这个问题，现在有了YARN操作系统。（YARN是一个仿照UC Berkeley AMPLab的Mesos做的系统。关于这个“模仿”，又有另一个故事。）</p>
<h2 id="plsa和mpi大数据的首要目标是大而不是快">pLSA和MPI:大数据的首要目标是大而不是快</h2>
<p>我2007年毕业后加入Google做研究。我们有一个同事叫张栋，他的工作涉及pLSA模型的并行化。这个课题很有价值，因为generalized matrix decomposition实际上是collaborative filtering的generalization，是用户行为分析和文本语义理解的共同基础。几年后的今天，我们都知道这是搜索、推荐和广告这三大互联网平台产品的基础。</p>
<p>当时的思路是用MPI来做并行化。张栋和宿华合作，开发一套基于MPI的并行pLSA系统。MPI是1980年代流行的并行框架，进入到很多大学的课程里，熟悉它的人很多。MPI这个框架提供了很多基本操作：除了点对点的Send, Recv，还有广播Bdcast，甚至还有计算加通信操作，比如AllReduce。</p>
<p>MPI很灵活，描述能力很强。因为MPI对代码结构几乎没有什么限制——任何进程之间可以在任何时候通信——所以很多人不称之为框架，而是称之为“接口”。</p>
<p>但是Google的并行计算环境上没有MPI。当时一位叫白宏杰的工程师将MPICH2移植到了Google的分布式操作系统上。具体的说，是重新实现MPI里的Send, Recv等函数，调用分布式操作系统里基于HTTP RPC的通信API。</p>
<p>MPI的AllReduce操作在很多机器学习系统的开发里都很有用。因为很多并行机器学习系统都是各个进程分别训练模型，然后再合适的时候（比如一个迭代结束的时候）大家对一下各自的结论，达成共识，然后继续迭代。这个“对一下结论，达成共识”的过程，往往可以通过AllReduce来完成。</p>
<p>如果我们关注一下MPI的研究，可以发现曾经有很多论文都在讨论如何高效实现AllReduce操作。比如我2008年的博文里提到一种当时让我们都觉得很聪明的一种算法。这些长年累月的优化，让MPICH2这样的系统的执行效率（runtime efficiency）非常出色。</p>
<p>基于MPI框架开发的pLSA模型虽然效率高，并且可以处理相当大的数据，但是还是不能处理Google当年级别的数据。原因如上节『概念』中所述——MPICH2没有自动错误恢复功能，而且MPI这个框架定义中提供的编程灵活性，让我们很难改进框架，使其具备错误恢复的能力。</p>
<p>具体的说，MPI允许进程之间在任何时刻互相通信。如果一个进程挂了，我们确实可以请分布式操作系统重启之。但是如果要让这个“新生”获取它“前世”的状态，我们就需要让它从初始状态开始执行，接收到其前世曾经收到的所有消息。这就要求所有给“前世”发过消息的进程都被重启。而这些进程都需要接收到他们的“前世”接收到过的所有消息。这种数据依赖的结果就是：所有进程都得重启，那么这个job就得重头做。</p>
<p>一个job哪怕只需要10分钟时间，但是这期间一个进程都不挂的概率很小。只要一个进程挂了，就得重启所有进程，那么这个job就永远也结束不了了。</p>
<p>虽然我们很难让MPI框架做到fault recovery，我们可否让基于MPI的pLSA系统支持fault recovery呢？原则上是可以的——最简易的做法是checkpointing——时不常的把有所进程接收到过的所有消息写入一个分布式文件系统（比如GFS）。或者更直接一点：进程状态和job状态写入GFS。Checkpointing是下文要说到的Pregel框架实现fault recovery的基础。</p>
<p>但是如果一个系统自己实现fault recovery，那还需要MPI做什么呢？做通信？——现代后台系统都用基于HTTP的RPC机制通信了，比如和Google的Stubby、Facebook的Thrift、腾讯的Poppy还有Go语言自带的rpc package。做进程管理？——在开源界没有分布式操作系统的那些年里有价值；可是今天（2013年），Google的Borg、AMPLab的Mesos和Yahoo!的YARN都比MPICH2做得更好，考虑更全面，效能更高。</p>
<h2 id="lda和mapreduce可扩展的基础是数据并行">LDA和MapReduce:可扩展的基础是数据并行</h2>
<p>因为MPI在可扩展性上的限制， 我们可以大致理解为什么Google的并行计算架构上没有实现经典的MPI。同时，我们自然的考虑Google里当时最有名的并行计算框架MapReduce。</p>
<p>MapReduce的风格和MPI截然相反。MapReduce对程序的结构有严格的约束——计算过程必须能在两个函数中描述：map和reduce；输入和输出数据都必须是一个一个的records；任务之间不能通信，整个计算过程中唯一的通信机会是map phase和reduce phase之间的shuffuling phase，这是在框架控制下的，而不是应用代码控制的。</p>
<p>pLSA模型的作者Thomas Hoffmann提出的机器学习算法是EM。EM是各种机器学习inference算法中少数适合用MapReduce框架描述的——map phase用来推测（inference）隐含变量的分布（distributions of hidden variables），也就是实现E-step；reduce phase利用上述结果来更新模型参数，也即是M-step。</p>
<p>但是2008年的时候，pLSA已经被新兴的LDA掩盖了。LDA是pLSA的generalization：一方面LDA的hyperparameter设为特定值的时候，就specialize成pLSA了。从工程应用价值的角度看，这个数学方法的generalization，允许我们用一个训练好的模型解释任何一段文本中的语义。而pLSA只能理解训练文本中的语义。（虽然也有ad hoc的方法让pLSA理解新文本的语义，但是大都效率低，并且并不符合pLSA的数学定义。）这就让继续研究pLSA价值不明显了。</p>
<p>另一方面，LDA的inference很麻烦，没法做精确inference，只有近似算法，比如variational inference。如果EM算法中的E-step采用variational infernece，那么EM算法就被称为variational EM。EM本来就是一个比较容易陷入局部最优的算法，E-step用了variational inference，总体效果就更差了。另一种近似inference算法是Gibbs sampling。虽然我们可以在E-step里用Gibbs sampling替换variational inference，但是Thomas Griffiths发现一个有趣的特点——稍微修改LDA之后，就可以利用Dirichlet和multinomial分布的共轭性，把模型参数都积分积掉。没有参数了，也就所谓M-step里的“更新参数”了。这样，只需要做Gibbs sampling即可。这个路子发表在PNAS上，题目是Finding Scientific Topics。</p>
<p>Gibbs sampling也有一个问题：作为一种Markov Chain Monte Carlo（MCMC）算法，顾名思义，Gibbs sampling是一个顺序过程，按照定义不能被并行化。幸运的是，2007年的时候，UC Irvine的David Newman团队发现，对于LDA这个特定的模型，Gibbs sampling可以被并行化。具体的说，把训练数据拆分成多份，用每一份独立的训练模型。每隔几个Gibbs sampling迭代，这几个局部模型之间做一次同步，得到一个全局模型，并且用这个全局模型替换各个局部模型。这个研究发表在NIPS上，题目是：Distributed Inference for Latent Dirichlet Allocation。</p>
<p>上述做法，在2012年Jeff Dean关于distributed deep leearning的<a href="https://research.google.com/archive/large_deep_networks_nips2012.html" target="_blank" rel="external">论文</a>中，被称为<strong>data parallelism</strong>（数据并行）。如果一个算法可以做数据并行，很可能就是可扩展（scalable）的了。</p>
<p>David Newman团队的发现允许我们用多个map tasks并行的做Gibbs sampling，然后在reduce phase中作模型的同步。这样，一个训练过程可以表述成一串MapReduce jobs。我用了一周时间在Google MapReduce框架上实现实现和验证了这个方法。后来在同事Matthew Stanton的帮助下，优化代码，提升效率。但是，因为每次启动一个MapReduce job，系统都需要重新安排进程（re-schedule）；并且每个job都需要访问GFS，效率不高。在当年的Google MapReduce系统中，1/3的时间花在这些杂碎问题上了。后来实习生司宪策在Hadoop上也实现了这个方法。我印象里Hadoop环境下，杂碎事务消耗的时间比例更大。</p>
<p>随后白红杰在我们的代码基础上修改了数据结构，使其更适合MPI的AllReduce操作。这样就得到了一个高效率的LDA实现。我们把用MapReduce和MPI实现的LDA的Gibbs sampling算法发表在这篇论文里了。</p>
<p>当我们踌躇于MPI的扩展性不理想而MapReduce的效率不理想时，Google MapReduce团队的几个人分出去，开发了一个新的并行框架Pregel。当时Pregel项目的tech lead访问中国。这个叫Grzegorz Malewicz的波兰人说服了我尝试在Pregel框架下验证LDA。但是在说这个故事之前，我们先看看Google Rephil——另一个基于MapReduce实现的并行隐含语义分析系统。</p>
<h2 id="rephil和mapreduce描述长尾数据的数学模型">Rephil和MapReduce：描述长尾数据的数学模型</h2>
<p>Google Rephil是Google AdSense背后广告相关性计算的头号秘密武器。但是这个系统没有发表过论文。只是其作者（博士Uri Lerner和工程师Mike Yar）在2002年在湾区举办的几次小规模交流中简要介绍过。所以Kevin Murphy把这些内容写进了他的书《Machine Learning: a Probabilitic Perspecitve》里。在吴军博士的《数学之美》里也提到了Rephil。</p>
<p>Rephil的模型是一个全新的模型，更像一个神经元网络。这个网络的学习过程从Web scale的文本数据中归纳海量的语义——比如“apple”这个词有多个意思：一个公司的名字、一种水果、以及其他。当一个网页里包含”apple”, “stock”, “ipad”等词汇的时候，Rephil可以告诉我们这个网页是关于apple这个公司的，而不是水果。</p>
<p>这个功能按说pLSA和LDA也都能实现。为什么需要一个全新的模型呢？</p>
<p>从2007年至今，国内外很多团队都尝试过并行化pLSA和LDA。心灵手巧的工程师们，成功的开发出能学习数万甚至上十万语义（latent topics）的训练系统。但是不管大家用什么训练数据，都会发现，得到的大部分语义（相关的词的聚类）都是非常类似，或者说“重复”的。如果做一个“去重”处理，几万甚至十万的语义，就只剩下几百几千了。</p>
<p>这是怎么回事？</p>
<p>如果大家尝试着把训练语料中的低频词去掉，会发现训练得到的语义和用全量数据训练得到的差不多。换句话说，pLSA和LDA模型的训练算法没有在意低频数据。</p>
<p>为什么会这样呢？因为pLSA和LDA这类概率模型的主要构造单元都是指数分布（exponential distributions）。比如pLSA假设一个文档中的语义的分布是multinomial的，每个语义中的词的分布也是multinomial的。因为multinomial是一种典型的指数分布，这样整个模型描述的海量数据的分布，不管哪个维度上的marginalization，都是指数分布。在LDA中也类似——因为LDA假设各个文档中的语义分布的multinomial distributions的参数是符合Dirichlet分布的，并且各个语义中的词的分布的multinomial distributions的参数也是符合Dirichlet分布的，这样整个模型是假设数据是指数分布的。</p>
<p>可是Internet上的实际数据基本都不是指数分布的——而是长尾分布的。至于为什么是这样？可以参见2006年纽约时报排名畅销书The Long Tail: Why the Future of Business is Selling Less of More。或者看看其作者Chris Anderson的博客<a href="http://www.thelongtail.com/" target="_blank" rel="external">The Long Tail</a>。</p>
<p>长尾分布的形状大致如下图所示：</p>
<div class="figure">
<img src="http://www.thelongtail.com/conceptual.jpg">

</div>
<p>其中x轴表示数据的类型，y轴是各种类型的频率，少数类型的频率很高（称为大头，图中红色部分），大部分很低，但是大于0（称为长尾，图中黄色部分）。一个典型的例子是文章中词的分布，有个具体的名字Zipf’s law，就是典型的长尾分布。而指数分布基本就只有大头部分——换句话说，如果我们假设长尾数据是指数分布的，我们实际上就把尾巴给割掉了。</p>
<p>割掉数据的尾巴——这就是pLSA和LDA这样的模型做的——那条长尾巴覆盖的多种多样的数据类型，就是Internet上的人生百态。理解这样的百态是很重要的。比如百度和Google为什么能如此赚钱？因为互联网广告收益。传统广告行业，只有有钱的大企业才有财力联系广告代理公司，一帮西装革履的高富帅聚在一起讨论，竞争电视或者纸媒体上的广告机会。互联网广告里，任何人都可以登录到一个网站上去投放广告，即使每日广告预算只有几十块人民币。这样一来，刘备这样织席贩屡的小业主，也能推销自己做的席子和鞋子。而搜索引擎用户的兴趣也是百花齐放的——从人人爱戴的陈老师苍老师到各种小众需求包括“红酒木瓜汤”（一种丰胸秘方，应该出丰胸广告）或者“苹果大尺度”（在搜索范冰冰主演的《苹果》电影呢）。把各种需求和各种广告通过智能技术匹配起来，就酝酿了互联网广告的革命性力量。这其中，理解各种小众需求、长尾意图就非常重要了。</p>
<p>实际上，Rephil就是这样一个能理解百态的模型。因为它把Google AdSense的盈利能力大幅提升，最终达到Google收入的一半。两位作者荣获Google的多次大奖，包括Founders’ Award。</p>
<p>而切掉长尾是一个很糟糕的做法。大家还记得小说《1984》里有这样一个情节吗？老大哥要求发布“新话”——一种新的语言，删掉自然英语中大部分词汇，只留下那些主流的词汇。看看小说里的人们生活的世界，让人浑身发毛，咱们就能体会“割尾巴”的恶果了。没有看过《1984》的朋友可以想象一下水木首页上只有“全站十大”，连“分类十大”都删掉之后的样子。</p>
<p>既然如此，为什么这类模型还要假设数据是指数分布的呢？——实在是不得已。指数分布是一种数值计算上非常方便的数学元素。拿LDA来说，它利用了Dirichlet和multinomial两种分布的共轭性，使得其计算过程中，模型的参数都被积分给积掉了（integrated out）。这是AD-LDA这样的ad hoc并行算法——在其他模型上都不好使的做法——在LDA上好用的原因之一。换句话说，这是为了计算方便，掩耳盗铃地假设数据是指数分布的。</p>
<p>实际上，这种掩耳盗铃在机器学习领域很普遍。比如有个兄弟听了上面的故事后说：“那我们就别用概率模型做语义分析了，咱们还用矩阵分解吧？SVD分解怎么样？” 很不好意思的，当我们把SVD分解用在语义分析（称为LSA，latent semantic analysis）上的时候，我们还是引入了指数分布假设——Gaussian assumption或者叫normality assumption。这怎么可能呢？SVD不就是个矩阵分解方法吗？确实传统SVD没有对数据分布的假设，但是当我们用EM之类的算法解决存在missing data的问题——比如LSA，还有推荐系统里的协同过滤（collaborative filtering）——这时不仅引入了Gaussian assumption，而且引入了linearity assumption。当我们用其他很多矩阵分解方法做，都存在同样的 问题。</p>
<p>掩耳盗铃的做法怎么能存在得如此自然呢？这是因为指数分布假设（尤其是Gaussian assumption）有过很多成功的应用，包括通信、数据压缩、制导系统等。这些应用里，我们关注的就是数据中的低频部分；而高频部分（或者说距离mean比较远的数据）即使丢掉了，电话里的声音也能听懂，压缩还原的图像也看得明白，导弹也还是能沿着“最可能”靠谱的路线飞行。我们当然会假设数据是指数分布的，这样不仅省计算开销，而且自然的忽略高频数据，我们还鄙夷地称之为outlier或者noise。</p>
<p>可是在互联网的世界里，正是这些五花八门的outliers和noise，蕴含了世间百态，让数据不可压缩，从而产生了“大数据”这么个概念。处理好大数据的公司，赚得盆满钵满，塑造了一个个传奇。这里有一个听起来比较极端的说法大数据里无噪声——很多一开始频率很低，相当长尾，会被词过滤系统认为是拼写错误的queries，都能后来居上成为主流。比如“神马”，“酱紫”。</p>
<p>Rephil系统实现的模型是一个神经元网络模型（neural network）。它的设计的主要考虑，就是要能尽量好的描述长尾分布的文本数据和其中蕴含的语义。Rephil模型的具体技术细节因为没有在论文中发表过，所以不便在这里透露。但是Rephil模型描述长尾数据的能力，是下文将要介绍的Peacock系统的原动力，虽然两者在模型上完全不同。</p>
<p>Rephil系统是基于Google MapReduce构建的。如上节所述，MapReduce在用来实现迭代算法的时候，效率是比较低的。这也是Peacock要设计全新框架的原动力——使其比MapReduce高效，但同时像MapReduce一样支持fault recovery。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文转载自 &lt;a href=&quot;http://www.flickering.cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;火光摇曳&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;大数据带来的新机遇&quot;&gt;大数
    
    </summary>
    
      <category term="Distributed Computing" scheme="http://www.shesong.org/categories/Distributed-Computing/"/>
    
      <category term="Machine Learning" scheme="http://www.shesong.org/categories/Distributed-Computing/Machine-Learning/"/>
    
    
      <category term="Distributed machine learning" scheme="http://www.shesong.org/tags/Distributed-machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>复杂网络调研</title>
    <link href="http://www.shesong.org/2017/02/22/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C%E8%B0%83%E7%A0%94/"/>
    <id>http://www.shesong.org/2017/02/22/复杂网络调研/</id>
    <published>2017-02-22T03:10:08.000Z</published>
    <updated>2017-03-23T08:19:19.130Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相关术语">0 相关术语</h1>
<table>
<colgroup>
<col width="27%">
<col width="62%">
<col width="10%">
</colgroup>
<thead>
<tr class="header">
<th align="left">Terms</th>
<th>Interpretation</th>
<th align="right">Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regular Network</td>
<td>Each Vertices has exactly the same number of neighbors.</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Random Network</td>
<td>A theoretical construct which contains links that are chosen completely at random with equal probability.</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">Geodesic Path(测地路径) or Shortest Path(最短路径)</td>
<td>A geodesic path is the shortest path through the network from one vertex to another. Note that there may be and often is more than one geodesic path between two vertices. [6]</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Geodesic Distance(测地距离) or Hop Distance(跳跃距离)</td>
<td>连接顶点 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> 的最短路径上的边的数目</td>
<td align="right"><span class="math inline">\(d_{i,j}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Eccentricity</td>
<td>The maximal distance bewteen vertex <span class="math inline">\(i\)</span> and vertex <span class="math inline">\(j\)</span></td>
<td align="right"><span class="math inline">\(\varepsilon_{i,j}=\max\{d_{i,j}\}\)</span></td>
</tr>
<tr class="even">
<td align="left">Strength(强度)</td>
<td>如果网络 <span class="math inline">\(G\)</span> 是无向加权网络，其权值矩阵 <span class="math inline">\(\mathbf{W}=(w_{ij})\)</span>，则顶点 <span class="math inline">\(i\)</span> 的强度定义为 <span class="math inline">\(s_i\)</span></td>
<td align="right"><span class="math inline">\(s_i=\sum\limits_{j=1}^Nw_{ij}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Density(密度)</td>
<td>网络中实际存在的边数 <span class="math inline">\(M\)</span> 与最大可能的边数之比</td>
<td align="right"><span class="math inline">\(\rho=\frac{M}{\frac{1}{2}N(N-1)}\)</span></td>
</tr>
<tr class="even">
<td align="left">Diameter(直径)</td>
<td>The diameter of a (undirected) network is the length (in number of edges) of the longest geodesic path between any two vertices. [6]</td>
<td align="right"><span class="math inline">\(D=\max\limits_{i,j}d_{i,j}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Radius(半径)</td>
<td>The minimal eccentricity of all vertices</td>
<td align="right"><span class="math inline">\(rad=\min\limits_{i,j} \varepsilon_{i,j}\)</span></td>
</tr>
<tr class="even">
<td align="left">Characteristic path length(特征路径长度) or Mean Geodesic Distance</td>
<td>任意两个节点之间的距离的平均值.</td>
<td align="right"><span class="math inline">\(\ell=\frac{1}{\frac{1}{2}n(n-1)}\sum\limits_{i\geq j}d_{i,j}\)</span></td>
</tr>
<tr class="odd">
<td align="left">The Average Degree</td>
<td>网络中所有节点 <span class="math inline">\(i\)</span> 的度 <span class="math inline">\(k_i\)</span> 的平局值.</td>
<td align="right"><span class="math inline">\(\left\langle k\right\rangle=\frac{1}{n}\sum \limits_ik_i\)</span></td>
</tr>
<tr class="even">
<td align="left">Pivotal vertices(关键节点)</td>
<td>A node is said to be pivotal if it lies on all shortest paths between two other nodes in the network.</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">h-Neighborhood</td>
<td>The set of all vertices <span class="math inline">\(j\)</span> with distance less than or equal to <span class="math inline">\(h\)</span> from <span class="math inline">\(i\)</span></td>
<td align="right"><span class="math inline">\(N_h(i)=\{j\in V \mid d_{i,j}\leq h\}\)</span></td>
</tr>
<tr class="even">
<td align="left">(Absolute) Hop Plot</td>
<td>The number of pairs <span class="math inline">\((i,j)\)</span> with <span class="math inline">\(d_{i,j}\leq h\)</span> to any parameter <span class="math inline">\(h\)</span></td>
<td align="right"><span class="math inline">\({\small P(h)=\mid\{(i,j)\in V^2 \mid d_{i,j}\leq h\}\mid}\)</span></td>
</tr>
<tr class="odd">
<td align="left">(Relative) Hop Plot</td>
<td>The fraction of pairs with a distance less than or equal to <span class="math inline">\(h\)</span></td>
<td align="right"><span class="math inline">\({\small p(h)=\frac{P(h)}{n^2}}\)</span></td>
</tr>
<tr class="even">
<td align="left">Effective Diameter(有效直径)</td>
<td>This is the minimum number of hops in which some fraction (say <span class="math inline">\(r=90\%\)</span>) [8] of all connected pairs of vertices can reach each other. [9]</td>
<td align="right"><span class="math inline">\({\small\min\{h\mid P(h)\geq rn^2\}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Clustering Coefficient(聚类系数)</td>
<td>Measures the extent to which my friends are friends with one another.</td>
<td align="right"><span class="math inline">\(C\)</span></td>
</tr>
<tr class="even">
<td align="left">Communities</td>
<td>Groups of vertices that have a high density of edges within them, with a lower density of edges between groups.</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">Resilience</td>
<td>The resilience of a graph is a measure of its robustness to node or edge failures. [14]</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Distribution function</td>
<td>The probability that a vertex chosen uniformly at random has degree <span class="math inline">\(k\)</span>.</td>
<td align="right"><span class="math inline">\(p_k\)</span></td>
</tr>
<tr class="odd">
<td align="left">Scale-Free(无标度)</td>
<td>Refers to any functional form <span class="math inline">\(f(k)\)</span> that remains unchanged to within a multiplicative factor under a rescaling of the independent variable <span class="math inline">\(k\)</span>, where <span class="math inline">\(f(k)=p_k\propto k^{-\alpha}\)</span> for some constant exponenet <span class="math inline">\(\alpha\)</span> It’s synonymous with “<strong>power-law</strong>”.</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Component</td>
<td>The component to which a vertex belongs is that set of vertices that can be reached from it by paths running along edges of the graph.</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">Betweenness Centrality(介数中心性)</td>
<td>The betweenness centrality of a vertex <span class="math inline">\(i\)</span> is the number of geodesic paths between other vertices that run through <span class="math inline">\(i\)</span>.</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Closeness Centrality(紧密度中心性)</td>
<td>Closeness centrality measures the mean distance from a vertex to other vertices.</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p><strong>Notes</strong>:</p>
<blockquote>
<p>For arbitrary edge weights, the problem of finding a geodesic path is <strong><span class="math inline">\(\mathcal{NP}\)</span>-hard</strong> [7], but for more special (but also more common) cases the distance can be calculated in <strong>polynomial</strong> time by solving the <strong>all-pairs shortestpath problem</strong> (<strong>APSP</strong>) [2].</p>
</blockquote>
<hr>
<h1 id="model-network-topology-1">1 Model (Network Topology) [1]</h1>
<h2 id="regular-network">1.1 Regular Network</h2>
<ul>
<li>全局耦合网络(globally coupled network): 任意两个点之间都有边直接相连</li>
<li>最近邻耦合网络(nearest-neighbor coupled network): 其中的每一个节点只和周围的邻居节点相邻</li>
<li>星形耦合网络(star coupled network): 只有一个中心点，其余的 N-1个点都只与这个中心点连接，而它们彼此之间不连接</li>
</ul>
<h2 id="random-network">1.2 Random Network</h2>
<p>随机图中两个节点之间不论是否具有共同的邻居节点，其连接概率为 <span class="math inline">\(p\)</span>。由于每条边的出现与否都是独立的，随即图的度分布可用 Possion 分布 [3] 来表示 <span class="math display">\[ P(k)=\binom{n}{k}p^{k}(1-p)^{N-k}\approx \frac{\left\langle k\right\rangle^k e^{-\left\langle k\right\rangle} }{k!} \]</span></p>
<h2 id="small-world-network">1.3 Small-World Network</h2>
<p>小世界网络作为从完全规则网络向完全随即图的过度，人们提出多种小世界模型，其中著名的有WS小世界模型 [4]和NW小世界模型 [3]。WS小世界模型是在该过度过程中随机化重连边，而NW小世界模型这是随机化加边。</p>
<blockquote>
<p>随即图和WS小世界模型的一个共同特征就是网络的连接度分布可近似用Poisson分布来表示，该分布在度平均值 <span class="math inline">\(\left\langle k\right\rangle\)</span> 处有一峰值，然后呈指数快速衰减。这意味着当 <span class="math inline">\(k\geq\left\langle k\right\rangle\)</span> 时，度为 <span class="math inline">\(k\)</span> 的节点几乎不存在。因此，这类网络也称为均匀网络或指数网络 (exponential network)。</p>
</blockquote>
<h2 id="scale-free-network">1.4 Scale-Free Network</h2>
<p>该网络的连接度分布函数具有幂律形式，现称为BA模型 [5]。该模型具有以下特征：</p>
<ul>
<li>增长 (growth) 特性：即网络的规模是不断扩大的。</li>
</ul>
<blockquote>
<p>WWW上每天都有大量新的网页产生</p>
</blockquote>
<ul>
<li>优先连接 (preferential attachment) 特性：即新的节点更倾向于与那些具有较高连接度的“大”节点相连接。这种现象也称为“富者更富 (rich get richer)”或“马太效应 (Matthew effect)”。</li>
</ul>
<blockquote>
<p>新发表的文章更倾向于引用一些已被广泛引用的重要文献。</p>
</blockquote>
<hr>
<h1 id="properties">2 Properties</h1>
<h2 id="small-world-effect">2.1 Small-World Effect</h2>
<blockquote>
<p>Most pairs of vertices in most networks seem to be connected by a short path through the network.</p>
</blockquote>
<p>The small-world effect has obvious implications for the dynamics of processes taking place on networks:</p>
<ul>
<li>If one considers the spread of information, or indeed anything else, across a network, the small-world effect implies that that spread will be fast on most real-world networks.</li>
<li>If it takes only six steps for a rumor to spread from any person to any other, for instance, then the rumor will spread much faster than if it takes a hundred steps, or a million.</li>
<li>This affects the number of “hops” a packet must make to get from one computer to another on the Internet.</li>
<li>The number of legs of a journey for an air or train traveler.</li>
<li>The time it takes for a disease to spread throughout a population.</li>
</ul>
<p><strong>Networks are said to show the small-world effect if the value of <span class="math inline">\(\ell\)</span> scales logarithmically or slower with network size for fixed mean degree.</strong></p>
<h2 id="transitivity横截性-or-clustering-6">2.2 Transitivity(横截性) or Clustering [6]</h2>
<p>The friend of your friend is likely also to be your friend. In terms of network topology, transitivity means the presence of a heightened number of triangles in the network—sets of three vertices each of which is connected to each of the others.</p>
<p>It can be quantified by defining a <strong>clustering coefficient</strong> <span class="math inline">\(C\)</span>, which can be written in three forms:</p>
<ol style="list-style-type: decimal">
<li><p>In this case, <span class="math inline">\(C\)</span> measures the fraction of triples that have their third edge filled in to complete the triangle: <span class="math display">\[ \begin{align} 
C = \frac{3\times \text{number of triangles in the network}}{\text{number of connected triples of vertices}}
\end{align} \]</span> where a “connected triple” means a single vertex with edges running to an unordered pair of others</p></li>
<li><p>In this case, <span class="math inline">\(C\)</span> is the mean probability that two vertices that are network neighbors of the same other vertex will themselves be neighbors, and also mean <strong>probability that the friend of your friend is also your friend</strong>. <span class="math display">\[ \begin{align} 
C = \frac{6\times \text{number of triangles in the network}}{\text{number of paths of length two}}
\end{align} \]</span> where a path of length two refers to a directed path starting from a specified vertex.</p></li>
<li><p>In this case, it tends to <strong>weight</strong> the contributions of <strong>low-degree vertices</strong> more heavily, because such vertices have a small denominator in (3) and hence can give quite different results from (1).[4] <span class="math display">\[ \begin{align} 
C = \frac{1}{n}\sum_i C_i,\quad C_i = \frac{\text{number of triangles connected to vertex $i$}}{\text{number of triples centered on vertex $i$s}}
\end{align} \]</span></p></li>
</ol>
<h2 id="degree-distributions">2.3 Degree Distributions</h2>
<p>The way of presenting degree data is to make a plot of the cumulative distribution function(累积度分布) <span class="math display">\[ \begin{align}
P_k=\sum_{k&#39;=k}^\infty p_{k&#39;}
\end{align} \]</span> which is the probability that the degree is greater than or equal to <span class="math inline">\(k\)</span>.</p>
<blockquote>
<p>Such a plot has the advantage that all the original data are representd and also reduces the noise in the tail.</p>
</blockquote>
<p>A network appears to have power-law degree destributions if its cumulative degree distribution indicated by their approximately straight-line forms on the doubly logarithmic scales.</p>
<h2 id="community-structure">2.4 <a href="#cs">Community</a> Structure</h2>
<p>The distribution of edges is not only globally, but also <strong>locally inhomogeneous</strong>, with high <strong>concentrations</strong> of edges within special groups of vertices, and low concentrations between these groups. This feature of real networks is called <strong>community structure</strong>, or <strong>clustering</strong>.</p>
<blockquote>
<p><strong>Clustering和Community Detection区别</strong>： [19]<br>
聚类问题和网络社区划分问题虽然存在内在的一致性，但是在本质上二者并不完全等同。聚类分析问题中的每一个数据点是孤立存在的，只存在与其它数据点的距离信息或者相似度信息；而网络中的每一个节点具备聚类分析问题所不具备的网络拓扑结构性质。网络中的每一个节点不仅受到其邻居的影响，而且还要受到其它节点经过拓扑性质传递给它的影响，这也正是基于网络拓扑结构性质相似度构造方法优于基于节点局部信息相似度构造方法的原因所在。</p>
</blockquote>
<h2 id="network-resilience">2.5 Network <a href="#Resilience">Resilience</a></h2>
<p>There are a variety of different ways in which vertices can be removed. For example, one could remove vertices at random from a network, or one could target some specific class of vertices, such as those with the highest degrees.</p>
<p>The work[11] study the effect of vertex deletion:</p>
<ul>
<li><a href="#mgd">Mean Geodesic Distance</a> is almost entirely unaffected by ramdom vertex removal since most of the vertices have low degree.</li>
<li>Mean Geodesic Distance increases very sharply with the fraction of the highest degree vertices removed.</li>
</ul>
<p>Resilience can be related to the graph diameter: a graph whose diameter does not increase much on node or edge removal has higher resilience.</p>
<h2 id="largest-component">2.6 Largest <a href="#component">Component</a></h2>
<p>In some networks the size of the largest component is an important quantity. &gt; In a communication network like the Internet the size of the largest component represents the largest fraction of the network within which communication is possible and hence is a measure of the effectiveness of the network at doing its job.</p>
<h2 id="network-centrality-15">2.7 Network Centrality [15]</h2>
<h3 id="betweenness-centrality-16">2.7.1 <a href="#bc">Betweenness Centrality</a> [16]</h3>
<p>Betweenness centrality can be viewed as a measure of network resilience - it tells us how many geodesic paths will get longer when a vertex is removed from the network. 顶点 <span class="math inline">\(i\)</span>的介数定义为 <span class="math display">\[ \begin{align}
BC_i=\sum_{s\neq i\neq t}\frac{n_{st}^i}{g_{st}}
\end{align} \]</span> 其中，<span class="math inline">\(g_{st}\)</span>为从顶点<span class="math inline">\(s\)</span>到顶点<span class="math inline">\(t\)</span>的最短路径的数目，<span class="math inline">\(n_{st}^i\)</span>是从顶点<span class="math inline">\(s\)</span>到顶点<span class="math inline">\(t\)</span>的<span class="math inline">\(g_{st}\)</span>条最短路径中经过顶点<span class="math inline">\(i\)</span>的最短路径的数目。</p>
<p>Latora and Marchiori [12] [13] have considered the <em>harmonic mean distance</em> between a vertex and all others can be viewed as a measure of network resilience, indicating how much effect on path length the removal of a vertex will have.</p>
<h3 id="closeness-centrality-17">2.7.2 <a href="#cc">Closeness Centrality</a> [17]</h3>
<p>The <a href="#mgd">mean geodesic distance</a> for vertex <span class="math inline">\(i\)</span> is: <span class="math display">\[ \begin{align}
\ell_i = \frac{1}{n}\sum_j d_{i,j}
\end{align} \]</span> This quantity takes low values for vertices that are separated from others by only a short geodesic distance on average.</p>
<p>However, the mean geodesic distance <span class="math inline">\(\ell_i\)</span> gives low values to more central nodes and high values to less central ones. Therefore, researchers commonly calculate its inverse, called <strong>closeness centrality</strong>: <span class="math display">\[ \begin{align}
C_i=\frac{1}{\ell_i}=\frac{n}{\sum_j d_{i,j}}
\end{align} \]</span></p>
<blockquote>
<p>Network Centrality 还有很多其他形式的定义，例如：Eigenvector centrality, Katz centrality, PageRank centrality, Cross-clique centrality, Percolation centrality, etc [18].</p>
</blockquote>
<h2 id="network-navigation">2.8 Network Navigation</h2>
<p>Stanley Milgram’s famous small-world experiment’s [10] results demonstrate that there exist short paths in the network, but they also demonstrate that ordinary people are good at finding them.</p>
<p>If it were possible to construct artificial networks that were easy to navigate in the same way that social networks appear to be, it has been suggested they could be used to build efficient database structures or better peer-to-peer computer networks</p>
<hr>
<h1 id="application">3 Application</h1>
<h2 id="anomaly-detection-20">3.1 Anomaly Detection [20]</h2>
<ul>
<li><p><strong>Definition</strong><br>
<em>Anomalies</em> are patterns in data that do not conform to a well defined notion of normal behavior.</p></li>
<li><strong>Research Areas</strong>
<ul>
<li><em>Statistics</em></li>
<li><em>Machine learning</em></li>
<li><em>Data mining</em></li>
<li><em>Information theory</em></li>
<li><em>Spectral theory</em></li>
</ul></li>
<li><strong>Practical Applications</strong>
<ul>
<li><em>Intrusion Detection</em></li>
<li><em>Fraud Detection</em></li>
<li><em>Medical and Public Health Anomaly Detection</em></li>
<li><em>Industrial Damage Detection</em></li>
<li><em>Image Processing</em></li>
<li><em>Anomaly Detection in Text Data</em></li>
<li><em>Sensor Networks</em></li>
</ul></li>
</ul>
<h2 id="link-analysis-21-22">3.2 Link Analysis [21] [22]</h2>
<p>Social interaction on the Web involves both positive and negative relationships — people form links to indicate friendship, support, or approval; but they also link to signify disapproval of others, or to express disagreement or distrust of the opinions of others.</p>
<p>Link Analysis provide insight into some of the fundamental principles that drive the formation of signed links in networks, shedding light on theories of balance and status from social psychology; they also suggest social computing applications by which the attitude of one user toward another can be estimated from evidence provided by their relationships with other members of the surrounding social network.</p>
<h2 id="recommender-systems-23">3.3 Recommender Systems [23]</h2>
<ul>
<li><strong>Social Network</strong> [24] [25] [27] [28] [29]</li>
<li><strong>Graphical Model</strong> [26]</li>
</ul>
<h2 id="deep-learning-for-graph-30-31">3.4 Deep Learning for Graph [30] [31]</h2>
<h2 id="node-representation-and-classification">3.5 Node Representation and Classification</h2>
<ul>
<li><p><strong>Node Classification</strong><br>
When dealing with large graphs, such as those that arise in the context of online social networks, a subset of nodes may be labeled. These labels can indicate demographic values, interest, beliefs or other characteristics of the nodes (users). A core problem is to use this information to extend the labeling so that all nodes are assigned a label (or labels). [32] [33] [34]</p></li>
<li><p><strong>Node Representation</strong><br>
Each vertex of the graph is represented with a low-dimensional vector in which meaningful semantic, relational and structural information conveyed by the graph can be accurately captured. [35] [36]</p></li>
</ul>
<h2 id="community-detection-37">3.6 Community Detection [37]</h2>
<h3 id="样例---社交网络算法在金融欺诈中的应用">3.6.0 样例 - 社交网络算法在金融欺诈中的应用</h3>
<ul>
<li>社交网络算法
<ul>
<li>分析指标：degree, <a href="#cc">closeness centrality</a>, <a href="#bc">betweenness centrality</a>, <a href="#clco">clustering coefficient</a>，等等。</li>
<li>算法：
<ul>
<li>PageRank</li>
<li>社区发现</li>
</ul></li>
</ul></li>
<li>在工业界的其他应用包括：精准营销，改善搜索/帮助推荐，网络系统安全</li>
</ul>
<p>ppt下载链接 <a href="http://pan.baidu.com/s/1pLay8UR" class="uri" target="_blank" rel="external">http://pan.baidu.com/s/1pLay8UR</a></p>
<h3 id="研究现状">3.6.1 研究现状</h3>
<p>目前流行的网络聚类方法是社区发现，已有的社区发现方法主要用来发现类内链接紧密、类间链接稀疏的网络结构(以下称该类结构为“<strong>传统社区</strong>”)。社区结构对深入理解网络拓扑结构、挖掘网络潜在模式、预测网络行为都具有十分重要的意义。而实际网络的结构比较复杂，人们预先并不知道网络中存在什么结构。</p>
<p>传统社区用于发现网络中<strong>紧密链接的聚类结构</strong>，根据节点链接紧密特性定义聚类中节点的相似性。当网络中不存在社区结构或存在其它结构时，传统社区发现方法不能有效识别网络的真实结构。</p>
<p>近来研究者提出采用一些<strong>概率生成模型</strong>，可发现网络中传统社区及之外的更多类型的聚类结构，该类聚类方法假设同类节点与它类具有相同的链接概率。基于该假设可刻画多种类型网络结构，如</p>
<ul>
<li><em>同类节点链接紧密、与异类节点链接稀疏的结构</em>(传统社区)；</li>
<li><em>与同类节点链接稀疏、与异类节点链接紧密的结构</em>(二分图或多分图)；</li>
<li><em>星型链接模式结构</em>。</li>
</ul>
<p>因此，该类方法可发现比传统社区更广义的聚类结构，以下统称为“<strong>广义社区</strong>”结构，发现这种结构的方法为广义社区发现方法。</p>
<p>传统社区发现的问题[2,3]：</p>
<ul>
<li>传统社区发现识别的聚类结构不准确。许多流行的传统社区发现方法基于模块度函数求解社区结构，存在分辨率和尺度问题，如社区发现结果易淹没小的社区。因此，该类方法的聚类结果不能反映实际网络的聚类事实。</li>
<li>传统社区发现在网络潜在结构不存在紧密链接子图结构时可能失效，也不能发现网络潜在的其它类型结构。在线社交平台的网络结构规律复杂，我们很难获取关于网络的先验知识，不知道网络是传统社区结构、二分图结构或其它多种类型结构的混合。大多传统社区发现方法假设网络中存在链接紧密的子图结构，且只能发现此类结构。</li>
<li>传统社区发现不能在发现网络社区结构的同时，识别出类间的交互规律：传统社区发现方法的目的是将网络节点按照链接紧密性聚类，不能提供社区间链接模式，不易于网络结构可视化及直观了解网络交互规律。</li>
</ul>
<hr>
<h3 id="复杂网络上的社区发现">3.6.2 复杂网络上的社区发现</h3>
<h4 id="社区发现分类">3.6.2.0 社区发现分类</h4>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/25/58b14aea858ce.png">

</div>
<h4 id="传统算法-traditional-methods">3.6.2.1 传统算法 (Traditional methods)</h4>
<h5 id="图分割-graph-partitioning">3.6.2.1.1 图分割 (Graph partitioning)</h5>
<p>社区可以看做密集子图结构，使用图分割算法来解决。图分割问题的目标是把图中的节点分成gg个预定大小的群组，这些群组之间的边数目最小，这个问题是NP-hard 的。</p>
<ul>
<li><strong>K-L算法（Kernighan-Lin algorithm）</strong>通过基于贪婪优化的启发式过程把网络分解为2个规模已知的社区。该算法为网络的划分引入一个增益函数，定义为两个社区内部的边数与两个社区边数之间的差，寻求Q的最大划分办法。[38]</li>
</ul>
<blockquote>
<p>K-L算法必须预先指定2个社区的大小，否则会得到错误的结果。这使得K-L算法无法应用于大多数真实网络。即使K-L算法的这一缺点得以克服，作为图分割方法，其先天性不足仍然难以解决。K-L算法在稀疏图中的时间复杂度是<span class="math inline">\(O(n^3)\)</span>。</p>
</blockquote>
<ul>
<li><strong>谱二分法（spectral bisection method）</strong>早期的分割都是二分图，社区发现也是基于二分的，遇到多分的情况就把其中一个子图再分割。比较经典的有谱二分法，利用拉普拉斯矩阵的第二小特征值<span class="math inline">\(\lambda_2\)</span>对社区二分类，这其实是属于谱方法的一种特例。[39] 谱二分法的步骤如下：
<ul>
<li>对于网络的相似度矩阵<span class="math inline">\(W=[w_{i,j}]\)</span>，计算对角矩阵 <span class="math inline">\(D_{i,i}=\sum\limits_j^Nw_{i,j}\)</span>，其中<span class="math inline">\(w_{i,j}\)</span></li>
<li>计算拉普拉斯（Laplacian）矩阵 <span class="math inline">\(L=D-W\)</span></li>
</ul></li>
</ul>
<blockquote>
<p>该拉普拉斯矩阵必有一个特征值为0，且对应的特征向量为<span class="math inline">\(\vec{1}=[1,1,\cdots,1]^{\rm T}\)</span>。而不为零的特征值所对应的特征向量的各元素中，同一个社区内的节点对应的元素是近似相等的。可以征明，除零特征值外，其它特 征值均大于零。谱二分法即是根据<span class="math inline">\(L\)</span>的第二个小特征值<span class="math inline">\(\lambda_2\)</span>：将网络分为两个社区。<span class="math inline">\(\lambda_2\)</span>被称为图的代数连接度，如果其值越小，谱二分法的效果就越好。谱二分法的主要缺点是只能将图分成2个子图，或者说偶数个子图。这使得人们在使用这种方法时，预先不能确定究竟将图分成多少个子图才合适。</p>
</blockquote>
<ul>
<li><strong>最大流（maximum flows）</strong>基于最大流的算法是G.W.Flake提出的。他给网络加了虚拟源节点ss和终点节点tt，并证明了经过最大流算法之后，包含源点ss的社区恰好满足社区内节点链接比与社区外的链接要多的性质。[40] [41]</li>
<li><strong>多层次图分割（level-structure partitioning）</strong>[42]</li>
</ul>
<h5 id="聚类-clustering">3.6.2.1.2 聚类 (Clustering)</h5>
<p>当社区的边非常密集，数目远大于点时，图分割可能就不太好使了，这时候社区发现可能更接近于聚类。我们把社区发现看做一组内容相似的物体集合，使用聚类算法。和图中的社区发现相比，图中的社区点与点之间可以用边来表示联系的紧密，而聚类中的社区，需要定义点之间的相似度，比如说根据邻接关系定义：<span class="math display">\[d_{ij}=\sqrt{\sum_{k\neq i,j}(a_{ik}-a_{jk})^2}\]</span>其中<span class="math inline">\(\mathbf{A}=(a_{ij})\)</span>为邻接矩阵，<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>的邻居越多，节点相似度越高。 聚类算法和网络发现（聚类相关的）算法可以很容易地互相转化。另外，社区发现可以是局部的，而聚类是全网络的。</p>
<ul>
<li><strong>层次聚类（hierarchical clustering）</strong>层次聚类假设社区是存在层次结构的（其实不一定额，可能是中心结构），计算网络中每一对节点的相似度。 然后分为凝聚法和分裂法两种：[43]
<ul>
<li><em>凝聚法（Agglomerative algorithms）</em>：根据相似度从强到弱连接相应节点对，形成树状图（Dendrogram），根据需求对树状图进行横切，获得社区结构。凝聚法的<strong>缺陷</strong>在于在某些应用中，当社区数目已经知道时，未必能得到正确的社区结构。另外，凝聚算法倾向于发现社区的核心，而忽略社区的外围。社区的核心部分往往与它周围点联系密切，因而易于发现．两社区的蒯边部分由于相对来说联系较少，所以很难划分。在一些情况下，某个点仅仅和特定的社区有一条边，凝聚算法很难正确划分该点。</li>
<li><em>分裂法（Divisive algorithms）</em>：找出相互关联最弱的节点，并删除他们之间的边，通过这样的反复操作将网络划分为越来越小的组件，连通的网络构成社区。</li>
</ul></li>
<li><strong>划分聚类（Partitional clustering）</strong>像k-means就很好，可以使用上面的相似度来聚类 。[44]</li>
<li><strong>谱聚类（Spectral clustering）</strong>图分割中的如 Ratio Cut和Normalized Cut其实和谱聚类是等价的，所以谱聚类也能用在社区发现上。</li>
<li><strong>局部聚类</strong></li>
</ul>
<h4 id="分裂方法divisive-algorithms">3.6.2.2 分裂方法（Divisive algorithms）</h4>
<p>这里的分裂法和层次聚类中的类似，区别是前者不计算节点相似度，而是删除是两个社区之间的关联边，这些边上的两点的相似度不一定很低。其中最著名的算法就是<strong>Girvan-Newman算法</strong>，根据以下假设：社区之间所存在的少数几个连接应该是社区间通信的瓶颈，是社区间通信时通信流量的必经之路。如果我们考虑网络中某种形式的通信并且寻找到具有最高通信流量（比如最小路径条数）的边，该边就应该是连接不同社区的通道。Girvan-Newman算法就是这样，迭代删除<strong>边介数（Edge Betweenness）</strong>最大的边。[45]</p>
<h4 id="基于模块度的方法modularity-based-methods">3.6.2.3 基于模块度的方法（Modularity-based methods）</h4>
<blockquote>
<p>Newman在自己2006年的文章里也证明了Modularity其实是对模块度矩阵进行一个谱分析。这也体现了Modularity方法的普适性和良好的可解释性。</p>
</blockquote>
<p>模块度不仅仅作为优化的目标函数提出，它也是目前是最流行的用来衡量社区结果好坏的标准之一（它的提出被称作社区发现研究历史上的里程碑)。我们知道，社区是节点有意识地紧密联系所造成的，它内部边的紧密程度总比一个随机的网络图来的紧密一些，模块度的定义就是基于此，它表示所有被划分到同一个社区的边所占的比例，再减除掉完全随机情况时被划分到同一个社区的边所占的比例： <span class="math display">\[ \begin{align}
Q=\frac{1}{2M}\sum_{ij}(a_{i,j}-\frac{k_ik_j}{2M})\delta(C_i,C_j)=\frac{1}{2M}\sum_{ij}b_{ij}\delta(C_i,C_j)
\end{align} \]</span> 其中<span class="math inline">\(\mathbf{A}=(a_{ij})\)</span>是网络的邻接矩阵，<span class="math inline">\(M\)</span>是整个图中边的数目，<span class="math inline">\(k_i\)</span> 是顶点<span class="math inline">\(i\)</span>的度数，<span class="math inline">\(C_i\)</span>表示节点<span class="math inline">\(i\)</span>所属的社区，如果两个节点属于同一个社区，则<span class="math inline">\(\delta\)</span>取值1，否则为0。注意公式（1） 的意义，<span class="math inline">\(\frac{a_{ij}}{2M}\)</span>是两个顶点之间连接的概率，如果我们保持一个网络的度分布但对其连边进行随机洗牌，任意一对节点在洗牌后存在连接的概率为<span class="math inline">\(\frac{k_ik_j}{2M}\)</span>。上式中括号表达的就是节点之间的实际连边概率高于期待值的程度。另<span class="math inline">\(\mathbf{B}=(b_{ij})_{N\times N}\)</span>也称为<strong>模块度矩阵</strong>（Modularity matrix）。</p>
<blockquote>
<p>modularity在large-scale network中有一个缺陷：resolution limit。在large network中，基于modularity的方法找不到那些small community，即便这些small community的结构都很明显。[58]</p>
</blockquote>
<p>公式还可以写成另一种形式： <span class="math display">\[ \begin{align}
Q=\sum_{i=1}^{n_c}(e_{ii}-a_i^2)
\end{align} \]</span> 其中 <span class="math inline">\(e_{ij}=\sum\limits_{vw}\frac{a_{vw}}{2M}\delta(C_i, C_j)\)</span>，且 <span class="math inline">\(e_{ii}\)</span> 是每个社团内区<span class="math inline">\(i\)</span>内部顶点之间的连边数占整个网络边数的比例，<span class="math inline">\(a_i=\frac{k_i}{2M}=\sum_je_{ij}\)</span>意味着一端与社团 <span class="math inline">\(i\)</span> 中顶点相连的连边的比例，<span class="math inline">\(n_c\)</span> 是整个网络中社区的数量。</p>
<p>在实际计算里，上式要求对社区及其内部节点进行遍历，这个计算复杂度是很大的。Newman(2006) [54]对上式进行了化简，得到矩阵表达如下： <span class="math display">\[ \begin{align}
Q=\frac{1}{2M}\sum_{ij}\sum_r\left[a_{ij}-\frac{k_ik_j}{2m}\right]S_{ir}S_{jr}=\frac{1}{2M}{\rm Tr}(\mathbf{S}^{\rm T}\mathbf{B}\mathbf{S})
\end{align} \]</span> 其中<span class="math inline">\(S\)</span>是<span class="math inline">\(N\times n_c\)</span>的矩阵，且如果顶点<span class="math inline">\(i\)</span>属于社区<span class="math inline">\(r\)</span>则<span class="math inline">\(S_{ir}\)</span>是1，否则为0.</p>
<p>模块度的一个优点是好坏与社区中点的数目无关。模块度真是个好东西，第一次对社区这个模糊的概念提出了量化的衡量标准（不过据说对于小粒度的不太准）。所以对模块度的算法优化多种多样，从贪心到模拟退火等应有尽有。</p>
<ul>
<li><strong>贪心策略（Greedy techniques）</strong> 有两个比较经典的算法，分别是
<ul>
<li>Fast greedy算法（CNN算法）[51]</li>
<li>Multilevel算法（Fast-Unfolding算法）[52] 该算法优点如下：（1）计算速度很快，可用于大规模网络。（2）是一种自下而上的凝聚过程，不会出现对小规模社团的探测遗漏现象，即解决了分辨率问题。（3）可应用于大规模的加权网络。 然而，在实际情况下，在顶点的直接邻近内的封闭社区可能是不准确的并且产生虚假伪分区。 因此，不清楚某些中间分区是否可以对应于图的有意义的分层级别。 此外，算法的结果取决于在顶点上的顺序扫描的顺序。</li>
</ul></li>
</ul>
<blockquote>
<p>Fast-Unfolding算法可以Spark中实现，已有现成的代码。[53]</p>
</blockquote>
<ul>
<li><strong>模拟退火（Simulated annealing）</strong></li>
<li><strong>极值优化（Extremal optimization）</strong></li>
<li><strong>谱优化（Spectral optimization）</strong></li>
</ul>
<h4 id="谱方法spectral-algorithms">3.6.2.4 谱方法（Spectral Algorithms）</h4>
<p>基于谱分析的社区算法基于如下事实，在同一个社区内的节点，它在拉普拉斯矩阵中的特征向量近似。将节点对应的矩阵特征向量（与特征值和特征向量有关的都叫谱）看成空间坐标，将网络节点映射到多维向量空间去，然后就可以运用传统的聚类算法将它们聚集成社团。这种方法不可避免的要计算矩阵的特征值，开销很大，但是因为能直接使用很多传统的向量聚类的成果，灵活性很高。</p>
<h4 id="动态算法dynamic-algorithms">3.6.2.5 动态算法（Dynamic Algorithms）</h4>
<p>自旋模型和同步算法应该是物理学家提出来的算法，话说物理学家在社区发现领域十分活跃，发了不少论文。随机游走是基于以下思想：如果存在很强的社区结构，那么随机游走器（random walker)会在社区内部停留更长的时间，因为社区内部的边密度比较高。</p>
<ul>
<li><strong>自旋模型（Spin models）</strong></li>
<li><strong>随机游走（Random walk）</strong></li>
<li><strong>同步算法（Synchronization）</strong></li>
</ul>
<h4 id="基于统计推断的算法methods-based-on-statistical-inference">3.6.2.6 基于统计推断的算法（Methods based on statistical inference）</h4>
<p>基于统计推断的方法包括观察到的数据集和对模型的假设。如果数据集是图，模型假设对节点之间如何联系的描述就要符合真实的图结构。[47] [48]</p>
<ul>
<li><strong>生成模型（Generative models）</strong></li>
<li><strong>判别模型（Blockmodeling, model selection and information theory）</strong> &gt; 赫赫有名的<strong>infomap</strong>这个方法是从编码的角度给了社区一个良好的解释，发现了网络的一种最优的二级编码就发现了对应的社区结构。思想是相当绝妙的，效果也很好。[49] [50]</li>
</ul>
<h4 id="总结">3.6.2.7 总结</h4>
<p>The networks have <span class="math inline">\(N\)</span> vetices and <span class="math inline">\(E\)</span> edges, and for each method the scale of networks means: S stands for small scale (<span class="math inline">\(&lt;10^4\)</span> vetices), M stands for medium scale (<span class="math inline">\(10^4-10^6\)</span> vetices), L stands for large scale (<span class="math inline">\(10^6-10^9\)</span> vetices). [61] [62]</p>
<table>
<thead>
<tr class="header">
<th>Algorithm</th>
<th align="center">Dense Network</th>
<th align="center">Sparse Network</th>
<th align="center">Scale</th>
<th align="center">Directed</th>
<th align="center">Weighted</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Edge betweenness</td>
<td align="center"><span class="math inline">\(O(E^2N)\)</span></td>
<td align="center"><span class="math inline">\(O(E^2N)\)</span></td>
<td align="center">S</td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>Fastgreedy</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(O(N\log^2 N)\)</span></td>
<td align="center">M</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>Infomap</td>
<td align="center"><span class="math inline">\(O(E)\)</span></td>
<td align="center"></td>
<td align="center">L</td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>Label propagation</td>
<td align="center"><span class="math inline">\(O(E)\)</span></td>
<td align="center"></td>
<td align="center">L</td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>Leading eigenvector</td>
<td align="center"><span class="math inline">\(O(N(E+N))\)</span></td>
<td align="center"><span class="math inline">\(O(N^2)\)</span></td>
<td align="center">L</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Multilevel</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(O(N\log N)\)</span></td>
<td align="center">L</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>Spinglass</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(O(N^{3.2})\)</span></td>
<td align="center">S</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>Walktrap</td>
<td align="center"><span class="math inline">\(O(EN^2)\)</span></td>
<td align="center"><span class="math inline">\(O(N^2\log N)\)</span></td>
<td align="center">M</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\checkmark\)</span></td>
</tr>
</tbody>
</table>
<hr>
<h3 id="社区发现算法特征">3.6.3 社区发现算法特征</h3>
<h4 id="优化目标">3.6.3.1 优化目标</h4>
<p>有一些社区发现算法比如谱方法，K-L算法，以及基于最大流的社区发现方法等，给出明确的的目标函数，并提出算法来最优化目标函数。 常用的优化目标函数有：</p>
<h5 id="normailized-cutratio-cut和conductance">3.6.3.1.1 Normailized Cut、Ratio Cut和conductance</h5>
<p>记<span class="math inline">\(\pi=(C_1,C_2,\cdots,C_K)\)</span>表示网络的社区，同时满足<span class="math inline">\(C_i\bigcap C_j=\emptyset,\, \bigcup_{i=1}^KC_i=V\)</span>，<strong>规范化割集标准（Normailized Cut</strong>）和<strong>比例割集标准（Ratio Cut）</strong>定义为： <span class="math display">\[ \begin{align}
Ratio\, Cut(\pi)=\frac{1}{K}\sum_{i=1}^K\frac{cut(C_i,\overline{C_i})}{|C_i|}
\end{align} \]</span> <span class="math display">\[ \begin{align}
Normailized\, Cut(\pi)=\frac{1}{K}\sum_{i=1}^K\frac{cut(C_i,\overline{C_i})}{vol(C_i)}, vol(C_i)=\sum_{v\in C_i}k_v
\end{align} \]</span></p>
<p>其中 <span class="math inline">\(cut(C_i,\overline{C_i})\)</span> 表示两者之间的联系数目。两个目标函数的目标是使得社区之间的边数量最小化的同时，避免出现类似于只包含1个成员的小社区。</p>
<p>连通度(conductance)也是类似的定义:[59] <span class="math display">\[ \begin{align}
Conductance(C_i)=\frac{cut(C_i,\overline{C_i})}{\min\{vol(C_i),vol(\overline{C_i})\}}
\end{align} \]</span></p>
<h5 id="kernighan-lin-object">3.6.3.1.2 Kernighan-Lin object</h5>
<p>K-L目标函数旨在使两个相同大小的社区之间的边联系最小： <span class="math display">\[ \begin{align}
KL_{obj}(C_1,\ldots,C_k)=\sum_{i\neq j}A(C_i,C_j)
\end{align} \]</span> 其中<span class="math inline">\(A(C_i,C_j)=\sum_{u\in C_i,v\in C_j}A(u,v), |C_1|=|C_2|=\ldots=|C_k|\)</span></p>
<h5 id="modularity">3.6.3.1.3 Modularity</h5>
<p>模块度已在3.6.2.3节提过。</p>
<h4 id="粒度控制社区数目可不可控">3.6.3.2 粒度控制（社区数目可不可控）</h4>
<p>对于有层次的社区发现算法来说的，比如某些二分社区算法，是通过不断递归的划分子社区来获得预定的社区数目。而某些算法，像层次聚类和MCL（Markov clustering），基于概率模型的社区发现算法等，允许用户通过调节参数来间接控制输出社区的数目。</p>
<p>另一些算法，像模块度优化算法，它的社区数目是由优化函数决定的，不需要用户来设定社区的数目。</p>
<h4 id="规模">3.6.3.3 规模</h4>
<p>很多算法在设计的时候，并没有特别地考虑伸缩性，在面对整个Web以及大型社交网络时动辄百万甚至千万个点时效果不佳。比如G-N算法，需要计算即通过每条边的最短路径数目（edge betweeness)，复杂度相当高，像谱聚类算法，能处理10K个点和70M条边就不错了。</p>
<p>所以，有些算法比如Shingling算法等，使用的方法相对简单，从而能适合大规模的社区发现的运行要求。</p>
<h4 id="重叠社区">3.6.3.4 重叠社区</h4>
<p>很多社区发现算法，比如图分割算法，将整个网络划分为多个独立的社区结构。但是在现实中，许多网络并不存在绝对的彼此独立的社团结构，相反，它们是由许多彼此重叠互相关联的社团构成，比如说在社交网络中，一个人根据兴趣的不同，有可能属于多个不同的小组等。所以，很多类似派系过滤算法（CPM - Clique Percolation Method）[55]这样旨在发现重叠社区的算法也被不断地提出来。</p>
<h4 id="评价标准">3.6.3.5 评价标准</h4>
<h5 id="准确率召回率f1值">3.6.3.5.1 准确率，召回率，F1值</h5>
<p>一个大规模数据集合中检索文档的时，可把文档分成四组：系统检索到的相关文档（A），系统检索到的不相关文档（B），相关但是系统没有检索到的文档（C），不相关且没有被系统检索到的文档（D）： 准确度定义为： <span class="math display">\[ \begin{align}
pr=\frac{A}{A+C}
\end{align} \]</span></p>
<p>召回率定义为： <span class="math display">\[ \begin{align}
rc=\frac{A}{A+B}
\end{align} \]</span></p>
<p>F-measure是准确率和召回率协调之后的结果，定义为： <span class="math display">\[ \begin{align}
PWF=\frac{2\times pr \times rc}{pr+rc}
\end{align} \]</span></p>
<p>同理，社区也可以用这个概念。</p>
<h5 id="平均聚类纯度average-cluster-purity">3.6.3.5.2 平均聚类纯度（average cluster purity）</h5>
<p>假设算法发现了<span class="math inline">\(C=\{C_1,\ldots,C_K\}\)</span>个社区，我们假设社区<span class="math inline">\(C_i\)</span>有<span class="math inline">\(n_i\)</span>个点，，每个点分别为<span class="math inline">\(\{v_{1,i},\ldots,v_{n_i,i}\}\)</span>。令<span class="math inline">\(M_{l,i}\)</span>为<span class="math inline">\(v_{l,i}\)</span>真实归属的社区的标签，平均聚类纯度为定义为： <span class="math display">\[ \begin{align}
ACP=\frac{1}{k}\sum_{i=1}^k\sum_{l=1}^{n_i}\frac{\delta(dom_i\in M_{l,i})}{n_i}
\end{align} \]</span> 即社区<span class="math inline">\(C_i\)</span>中主要标签的点占社区所有点的数目比例。</p>
<h5 id="互信息">3.6.3.5.3 互信息</h5>
<p>首先来回顾熵的定义,在一个分布内包含的信息为熵： <span class="math display">\[ \begin{align}
H(X)=-\sum_{x \in X}p(x)\log p(x)
\end{align} \]</span></p>
<p><strong>互信息（mutual information)</strong>描述了两个分布之间的相关性： <span class="math display">\[ \begin{align}
I(X;Y)=H(X)-H(X|Y)=\sum_{y \in Y}\sum_{x \in X}p(x,y)\log(\frac{p(x,y)}{p(x)p(y)})
\end{align} \]</span> 但事实上，对于一个划分<span class="math inline">\(X\)</span>，任何一个从<span class="math inline">\(X\)</span>派生的划分<span class="math inline">\(Y\)</span>都和<span class="math inline">\(X\)</span>有相同的互信息，尽管这些划分都不尽相同。在这种情况下条件熵<span class="math inline">\(H(X|Y)\)</span>都近乎为0，这也导致了互信息<span class="math inline">\(I(X;Y)\)</span>几乎等于<span class="math inline">\(H(X)\)</span>。为了避免这种情况，Danon et al.提出了归一化互信息（normalized mutual information），直到现在都使用得非常广泛：[57] [60] <span class="math display">\[ \begin{align}
I_{norm}(\mathcal{X},\mathcal{Y})=\frac{2I(X;Y)}{H(X)+H(Y)}
\end{align} \]</span> 所谓两个事件相关性的量化度量，就是在了解其中一个<span class="math inline">\(Y\)</span>的前提下，对消除另一个<span class="math inline">\(X\)</span>不确定性所提供的信息量。 规范化的互信息定义为 <span class="math display">\[ \begin{align}
NMI(X;Y)=\frac{I(X;Y)}{\sqrt{H(X)H(Y)}}
\end{align} \]</span> 我们将划分当做一个结点落在社区的概率分布，然后计算社区划分结果和真实情况的NMI值，具体例子见参考《社会计算:社区发现和社会媒体挖掘》[56].</p>
<p>[1]: <a href="http://vdisk.weibo.com/s/ztDcAsY3JGuzD" target="_blank" rel="external">汪小帆, 李翔, and 陈关荣. 复杂网络理论及其应用. 清华大学出版社有限公司, 2006.</a></p>
<p>[2]: <a href="http://link.springer.com/book/10.1007/b106453" target="_blank" rel="external">Brandes, Ulrik, and Thomas Erlebach. Network analysis: methodological foundations. Vol. 3418. Springer Science &amp; Business Media, 2005.</a></p>
<p>[3]: <a href="http://link.springer.com/chapter/10.1007/978-1-4612-0619-4_7" target="_blank" rel="external">Bollobás, Béla. “Random graphs.” Modern Graph Theory. Springer New York, 1998. 215-252.</a></p>
<p>[4]: <a href="http://www.nature.com/nature/journal/v393/n6684/abs/393440a0.html" target="_blank" rel="external">Watts, Duncan J., and Steven H. Strogatz. “Collective dynamics of ‘small-world’networks.” nature 393.6684 (1998): 440-442.</a></p>
<p>[5]: <a href="http://science.sciencemag.org/content/286/5439/509" target="_blank" rel="external">Barabási, Albert-László, and Réka Albert. “Emergence of scaling in random networks.” science 286.5439 (1999): 509-512.</a></p>
<p>[6]: <a href="http://epubs.siam.org/doi/abs/10.1137/s003614450342480" target="_blank" rel="external">Newman, Mark EJ. “The structure and function of complex networks.” SIAM review 45.2 (2003): 167-256.</a></p>
<p>[7]: <a href="http://dl.acm.org/citation.cfm?id=578533" target="_blank" rel="external">Gary, Michael R., and David S. Johnson. “Computers and Intractability: A Guide to the Theory of NP-completeness.” (1979).</a></p>
<p>[8]: <a href="http://repository.cmu.edu/compsci/565/" target="_blank" rel="external">Palmer, Christopher R., et al. “The connectivity and fault-tolerance of the Internet topology.” (2001).</a></p>
<p>[9]: <a href="http://ieeexplore.ieee.org/abstract/document/965863/" target="_blank" rel="external">Tauro, Sudhir Leslie, et al. “A simple conceptual model for the internet topology.” Global Telecommunications Conference, 2001. GLOBECOM’01. IEEE. Vol. 3. IEEE, 2001.</a></p>
<p>[10]: <a href="http://www.jstor.org/stable/pdf/2786545.pdf" target="_blank" rel="external">Travers, Jeffrey, and Stanley Milgram. “An experimental study of the small world problem.” Sociometry (1969): 425-443.</a></p>
<p>[11]: <a href="http://www.nature.com/nature/journal/v406/n6794/abs/406378A0.html" target="_blank" rel="external">Albert, Réka, Hawoong Jeong, and Albert-László Barabási. “Error and attack tolerance of complex networks.” nature 406.6794 (2000): 378-382.</a></p>
<p>[12]: <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.87.198701" target="_blank" rel="external">Latora, Vito, and Massimo Marchiori. “Efficient behavior of small-world networks.” Physical review letters 87.19 (2001): 198701.</a></p>
<p>[13]: <a href="http://link.springer.com/article/10.1140%2Fepjb%2Fe2003-00095-5?LI=true" target="_blank" rel="external">Latora, Vito, and Massimo Marchiori. “Economic small-world behavior in weighted networks.” The European Physical Journal B-Condensed Matter and Complex Systems 32.2 (2003): 249-263.</a></p>
<p>[14]: <a href="http://dl.acm.org/citation.cfm?id=1132954" target="_blank" rel="external">Chakrabarti, Deepayan, and Christos Faloutsos. “Graph mining: Laws, generators, and algorithms.” ACM computing surveys (CSUR) 38.1 (2006): 2.</a></p>
<p>[15]: <a href="http://cs.brynmawr.edu/Courses/cs380/spring2013/section02/slides/05_Centrality.pdf" target="_blank" rel="external">Network Centrality Based on materials by Lada Adamic, UMichigan</a></p>
<p>[16]: <a href="http://www.jstor.org/stable/3033543" target="_blank" rel="external">Freeman, Linton C. “A set of measures of centrality based on betweenness.” Sociometry (1977): 35-41.</a></p>
<p>[17]: <a href="http://www.sci.unich.it/~francesc/teaching/network/closeness.html" target="_blank" rel="external">Closeness Centrality</a></p>
<p>[18]: <a href="https://en.wikipedia.org/wiki/Centrality" target="_blank" rel="external">Centrality - Wikipedia</a></p>
<p>[19]: <a href="http://f.wanfangdata.com.cn/download/Thesis_Y2734231.aspx" target="_blank" rel="external">姜雅文. 复杂网络社区发现若干问题研究. Diss. 北京交通大学, 2014.</a></p>
<p>[20]: <a href="http://dl.acm.org/citation.cfm?id=1541882" target="_blank" rel="external">Chandola, Varun, Arindam Banerjee, and Vipin Kumar. “Anomaly detection: A survey.” ACM computing surveys (CSUR) 41.3 (2009): 15.</a></p>
<p>[21]: <a href="http://dl.acm.org/citation.cfm?id=1772756" target="_blank" rel="external">Leskovec Jure, Daniel Huttenlocher, and Jon Kleinberg. “Predicting positive and negative links in online social networks.” Proceedings of the 19th international conference on World wide web. ACM, 2010.</a></p>
<p>[22]: <a href="https://arxiv.org/abs/1011.4071v1" target="_blank" rel="external">Backstrom, Lars, and Jure Leskovec. “Supervised random walks: predicting and recommending links in social networks.” Proceedings of the fourth ACM international conference on Web search and data mining. ACM, 2011.</a></p>
<p>[23]: <a href="http://dl.acm.org/citation.cfm?id=608330" target="_blank" rel="external">Mirza, Batul J., Benjamin J. Keller, and Naren Ramakrishnan. “Studying recommendation algorithms by graph analysis.” Journal of Intelligent Information Systems 20.2 (2003): 131-160.</a></p>
<p>[24]: <a href="http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf" target="_blank" rel="external">Haveliwala, Taher H. “Topic-sensitive pagerank.” Proceedings of the 11th international conference on World Wide Web. ACM, 2002.</a></p>
<p>[25]: <a href="http://www.sciencedirect.com/science/article/pii/S0020025513003149" target="_blank" rel="external">Durand, Guillaume, Nabil Belacel, and François LaPlante. “Graph theory based model for learning path recommendation.” Information Sciences 251 (2013): 10-21.</a></p>
<p>[26]: <a href="http://dl.acm.org/citation.cfm?id=2507163" target="_blank" rel="external">McAuley, Julian, and Jure Leskovec. “Hidden factors and hidden topics: understanding rating dimensions with review text.” Proceedings of the 7th ACM conference on Recommender systems. ACM, 2013.</a></p>
<p>[27]: <a href="http://dl.acm.org/citation.cfm?id=2556259" target="_blank" rel="external">Yu, Xiao, et al. “Personalized entity recommendation: A heterogeneous information network approach.” Proceedings of the 7th ACM international conference on Web search and data mining. ACM, 2014.</a></p>
<p>[28]: <a href="https://arxiv.org/abs/1609.08264v1" target="_blank" rel="external">Kang, Zhao, et al. “Top-N Recommendation on Graphs.” Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 2016.</a></p>
<p>[29]: <a href="http://dl.acm.org/citation.cfm?id=2959185" target="_blank" rel="external">Christakopoulou, Evangelia, and George Karypis. “Local Item-Item Models For Top-N Recommendation.” Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016.</a></p>
<p>[30]: <a href="http://dl.acm.org/citation.cfm?id=2623732" target="_blank" rel="external">Perozzi, Bryan, Rami Al-Rfou, and Steven Skiena. “Deepwalk: Online learning of social representations.” Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014.</a></p>
<p>[31]: <a href="https://arxiv.org/abs/1506.05163v1" target="_blank" rel="external">Henaff, Mikael, Joan Bruna, and Yann LeCun. “Deep convolutional networks on graph-structured data.” arXiv preprint arXiv:1506.05163 (2015).</a></p>
<p>[32]: <a href="https://arxiv.org/abs/1101.3291" target="_blank" rel="external">Bhagat, Smriti, Graham Cormode, and S. Muthukrishnan. “Node classification in social networks.” Social network data analytics. Springer US, 2011. 115-148.</a></p>
<p>[33]: <a href="http://web.eecs.umich.edu/~dkoutra/papers/fabp_pkdd2011.pdf" target="_blank" rel="external">Koutra, Danai, et al. “Unifying guilt-by-association approaches: Theorems and fast algorithms.” Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2011.</a></p>
<p>[34]: <a href="http://dl.acm.org/citation.cfm?id=2741093" target="_blank" rel="external">Tang, Jian, et al. “Line: Large-scale information network embedding.” Proceedings of the 24th International Conference on World Wide Web. ACM, 2015.</a></p>
<p>[35]: <a href="https://arxiv.org/abs/1607.00653" target="_blank" rel="external">Grover, Aditya, and Jure Leskovec. “node2vec: Scalable Feature Learning for Networks.”</a></p>
<p>[36]: <a href="http://dl.acm.org/citation.cfm?id=2806512" target="_blank" rel="external">Cao, Shaosheng, Wei Lu, and Qiongkai Xu. “Grarep: Learning graph representations with global structural information.” Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015.</a></p>
<p>[37]: <a href="http://www.sciencedirect.com/science/article/pii/S0370157309002841" target="_blank" rel="external">Fortunato, Santo. “Community detection in graphs.” Physics reports 486.3 (2010): 75-174.</a></p>
<p>[38]: <a href="http://ieeexplore.ieee.org/abstract/document/6771089/" target="_blank" rel="external">Kernighan, Brian W., and Shen Lin. “An efficient heuristic procedure for partitioning graphs.” The Bell system technical journal 49.2 (1970): 291-307.</a></p>
<p>[39]: <a href="http://epubs.siam.org/doi/abs/10.1137/0603056?journalCode=sjamdu" target="_blank" rel="external">Barnes, Earl R. “An algorithm for partitioning the nodes of a graph.” SIAM Journal on Algebraic Discrete Methods 3.4 (1982): 541-550.</a></p>
<p>[40]: <a href="http://dl.acm.org/citation.cfm?id=347121" target="_blank" rel="external">Flake, Gary William, Steve Lawrence, and C. Lee Giles. “Efficient identification of web communities.” Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2000.</a></p>
<p>[41]: <a href="http://ieeexplore.ieee.org/abstract/document/989932/" target="_blank" rel="external">Flake, Gary William, et al. “Self-organization and identification of web communities.” Computer 35.3 (2002): 66-70.</a></p>
<p>[42]: <a href="http://link.springer.com/chapter/10.1007/978-94-011-5412-3_12" target="_blank" rel="external">Pothen, Alex. “Graph partitioning algorithms with applications to scientific computing.” Parallel Numerical Algorithms. Springer Netherlands, 1997. 323-368.</a></p>
<p>[43]: <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf" target="_blank" rel="external">Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1. Springer, Berlin: Springer series in statistics, 2001.</a></p>
<p>[44]: <a href="https://projecteuclid.org/euclid.bsmsp/1200512992" target="_blank" rel="external">MacQueen, James. “Some methods for classification and analysis of multivariate observations.” Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. Vol. 1. No. 14. 1967.</a></p>
<p>[45]: <a href="http://www.pnas.org/content/99/12/7821.short" target="_blank" rel="external">Girvan, Michelle, and Mark EJ Newman. “Community structure in social and biological networks.” Proceedings of the national academy of sciences 99.12 (2002): 7821-7826.</a></p>
<p>[46]: <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.69.026113" target="_blank" rel="external">Newman, Mark EJ, and Michelle Girvan. “Finding and evaluating community structure in networks.” Physical review E 69.2 (2004): 026113.</a></p>
<p>[47]: <a href="http://www.cnki.com.cn/Article/CJFDTotal-JSJA201208000.htm" target="_blank" rel="external">柴变芳, 贾彩燕, and 于剑. “基于统计推理的社区发现模型综述.” 计算机科学 2012 年 08 (2012): 1-7+.</a></p>
<p>[48]: <a href="http://cdmd.cnki.com.cn/Article/CDMD-10004-1015611891.htm" target="_blank" rel="external">柴变芳. 基于生成模型的大规模网络广义社区发现方法研究. Diss. 北京交通大学, 2015.</a></p>
<p>[49]: <a href="http://www.mapequation.org/assets/publications/RosvallBergstromPNAS2008Full.pdf" target="_blank" rel="external">Rosvall, Martin, and Carl T. Bergstrom. “Maps of random walks on complex networks reveal community structure.” Proceedings of the National Academy of Sciences 105.4 (2008): 1118-1123.</a></p>
<p>[50]: <a href="https://arxiv.org/abs/0906.1405" target="_blank" rel="external">Rosvall, Martin, Daniel Axelsson, and Carl T. Bergstrom. “The map equation.” The European Physical Journal Special Topics 178.1 (2009): 13-23.</a></p>
<p>[51]: <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.70.066111" target="_blank" rel="external">Clauset, Aaron, Mark EJ Newman, and Cristopher Moore. “Finding community structure in very large networks.” Physical review E 70.6 (2004): 066111.</a></p>
<p>[52]: <a href="https://arxiv.org/abs/0803.0476v2" target="_blank" rel="external">Blondel, Vincent D., et al. “Fast unfolding of communities in large networks.” Journal of statistical mechanics: theory and experiment 2008.10 (2008): P10008.</a></p>
<p>[53]: <a href="http://bbs.pinggu.org/thread-3614747-1-1.html" target="_blank" rel="external">基于GraphX的社区发现算法FastUnfolding分布式实现</a></p>
<p>[54]: <a href="http://www.pnas.org/content/103/23/8577.short" target="_blank" rel="external">Newman, Mark EJ. “Modularity and community structure in networks.” Proceedings of the national academy of sciences 103.23 (2006): 8577-8582.</a></p>
<p>[55]: <a href="http://www.nature.com/nature/journal/v435/n7043/abs/nature03607.html" target="_blank" rel="external">Palla, Gergely, et al. “Uncovering the overlapping community structure of complex networks in nature and society.” Nature 435.7043 (2005): 814-818.</a></p>
<p>[56]: <a href="https://github.com/rexwong/dp/wiki/%E7%A4%BE%E4%BC%9A%E8%AE%A1%E7%AE%97-%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E5%92%8C%E7%A4%BE%E4%BC%9A%E5%AA%92%E4%BD%93%E6%8C%96%E6%8E%98" target="_blank" rel="external">LeiTang, and HuanLin. 社会计算:社区发现和社会媒体挖掘. 机械工业出版社, 2013.</a></p>
<p>[57]: <a href="http://iopscience.iop.org/article/10.1088/1742-5468/2005/09/P09008/meta" target="_blank" rel="external">Danon, Leon, et al. “Comparing community structure identification.” Journal of Statistical Mechanics: Theory and Experiment 2005.09 (2005): P09008.</a></p>
<p>[58]: <a href="http://www.pnas.org/content/104/1/36.short" target="_blank" rel="external">Fortunato, Santo, and Marc Barthelemy. “Resolution limit in community detection.” Proceedings of the National Academy of Sciences 104.1 (2007): 36-41.</a></p>
<p>[59]: <a href="http://dl.acm.org/citation.cfm?id=1772755" target="_blank" rel="external">Leskovec, Jure, Kevin J. Lang, and Michael Mahoney. “Empirical comparison of algorithms for network community detection.” Proceedings of the 19th international conference on World wide web. ACM, 2010.</a></p>
<p>[60]: <a href="https://arxiv.org/abs/0908.1062v2" target="_blank" rel="external">Lancichinetti, Andrea, and Santo Fortunato. “Community detection algorithms: a comparative analysis.” Physical review E 80.5 (2009): 056117.</a></p>
<p>[61]: <a href="http://www.nature.com/articles/srep30750" target="_blank" rel="external">Yang, Zhao, René Algesheimer, and Claudio J. Tessone. “A Comparative Analysis of Community Detection Algorithms on Artificial Networks.” Scientific Reports 6 (2016).</a></p>
<p>[62]: <a href="https://link.springer.com/article/10.1007/s10618-011-0224-z" target="_blank" rel="external">Papadopoulos, Symeon, et al. “Community detection in social media.” Data Mining and Knowledge Discovery 24.3 (2012): 515-554.</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;相关术语&quot;&gt;0 相关术语&lt;/h1&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&quot;27%&quot;&gt;
&lt;col width=&quot;62%&quot;&gt;
&lt;col width=&quot;10%&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&quot;he
    
    </summary>
    
      <category term="Complex Network" scheme="http://www.shesong.org/categories/Complex-Network/"/>
    
    
      <category term="调研" scheme="http://www.shesong.org/tags/%E8%B0%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>A MapReduce Algorithm for Matrix-Vector Multiplication</title>
    <link href="http://www.shesong.org/2016/10/17/A-MapReduce-Algorithm-for-Matrix-Vector-Multiplication/"/>
    <id>http://www.shesong.org/2016/10/17/A-MapReduce-Algorithm-for-Matrix-Vector-Multiplication/</id>
    <published>2016-10-17T01:35:24.000Z</published>
    <updated>2017-02-27T08:14:02.798Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题定义">1 问题定义</h3>
<p>假设有矩阵 <span class="math inline">\(\mathcal{A}=(a_{i,k})\in\mathbb{R}^{I\times K}\)</span>，其中 <span class="math inline">\(i\in[0,I),k\in[0,K)\)</span>，向量 <span class="math inline">\(\mathcal{B}=(b_{k,0})\in\mathbb{R}^{K\times 1}\)</span>，其中 <span class="math inline">\(k\in[0,K)\)</span>。我们作矩阵乘向量运算，可得向量 <span class="math inline">\(\mathcal{C}=\mathcal{A}\mathcal{B}=(c_{i,0})\in\mathbb{R}^{I\times 1}\)</span>，其中 <span class="math inline">\(c_{i,0}=\sum\limits_{k=0}^{K-1}a_{i,k}b_{k,0}\)</span>。当矩阵大到一定程度时，一台服务器由于内存限制已经无法处理。在此我们考虑基于分块的大矩阵乘法，并在Hadoop平台上实现该算法。</p>
<p>我们将 <span class="math inline">\(\mathcal{A}\)</span> 和 <span class="math inline">\(\mathcal{B}\)</span> 划分为足够小的块（子矩阵），使得 <span class="math inline">\(\mathcal{A}\)</span> 和 <span class="math inline">\(\mathcal{B}\)</span> 的小块可以在集群中的单个节点上的内存中相乘。设 <span class="math inline">\(IB\)</span> 表示分块后每个 <span class="math inline">\(\mathcal{A}\)</span> 块和 <span class="math inline">\(\mathcal{C}\)</span> 块的行数，<span class="math inline">\(KB\)</span> 表示每个 <span class="math inline">\(\mathcal{A}\)</span> 块的列数和 <span class="math inline">\(\mathcal{B}\)</span> 块的行数，<span class="math inline">\(NIB\)</span> 表示 <span class="math inline">\(\mathcal{A}\)</span> 行和 <span class="math inline">\(\mathcal{C}\)</span> 行的划分数，即 <span class="math inline">\(NIB=\frac{I-1}{IB}+1\)</span>，<span class="math inline">\(NKB\)</span> 表示 <span class="math inline">\(\mathcal{A}\)</span> 列和 <span class="math inline">\(\mathcal{B}\)</span> 行的划分数，即 <span class="math inline">\(NKB=\frac{K-1}{KB}+1\)</span>。</p>
<hr>
<h3 id="示例">2 示例</h3>
<p>矩阵 <span class="math inline">\(\mathcal{A}=\left[\begin{array}{cc|cc}a_{00} &amp; a_{01} &amp; a_{02} &amp; a_{03}\\a_{10} &amp; a_{11} &amp; a_{12} &amp; a_{13}\\\hline a_{20} &amp; a_{21} &amp; a_{22} &amp; a_{23}\\a_{30} &amp; a_{31} &amp; a_{32} &amp; a_{33}\\\hline a_{40} &amp; a_{41} &amp; a_{42} &amp; a_{43}\\a_{50} &amp; a_{51} &amp; a_{52} &amp; a_{53}\\\end{array}\right]\)</span> 被分成 <span class="math inline">\(3\times 2\)</span> 个块，记为 <span class="math inline">\(\mathcal{A}=\left[\begin{array}{cc}\mathbf{A}_{00} &amp; \mathbf{A}_{01}\\\mathbf{A}_{10} &amp; \mathbf{A}_{11}\\\mathbf{A}_{20} &amp; \mathbf{A}_{21}\\\end{array}\right]\)</span>。</p>
<p>向量 <span class="math inline">\(\mathcal{B}=\left[\begin{array}{c}b_{00}\\b_{10}\\\hline b_{20}\\b_{30}\end{array}\right]\)</span> 被分为 <span class="math inline">\(2\times 1\)</span> 个块，记为 <span class="math inline">\(\mathcal{B}=\left[\begin{array}{c}\mathbf{B}_{00}\\\mathbf{B}_{10}\end{array}\right]\)</span>。</p>
<p>则矩阵 <span class="math inline">\(\mathcal{C}=\mathcal{A}\mathcal{B}=\left[\begin{array}{c}\mathbf{C}_{00}\\\mathbf{C}_{10}\\\mathbf{C}_{20}\end{array}\right]=\left[\begin{array}{c}\mathbf{A}_{00}\mathbf{B}_{00}\\\mathbf{A}_{10}\mathbf{B}_{00}\\\mathbf{A}_{20}\mathbf{B}_{00}\end{array}\right]+\left[\begin{array}{c}\mathbf{A}_{01}\mathbf{B}_{10}\\\mathbf{A}_{11}\mathbf{B}_{10}\\\mathbf{A}_{21}\mathbf{B}_{10}\end{array}\right]=\left[\begin{array}{c}\mathbf{A}_{00}\mathbf{B}_{00}+\mathbf{A}_{01}\mathbf{B}_{10}\\\mathbf{A}_{10}\mathbf{B}_{00}+\mathbf{A}_{11}\mathbf{B}_{10}\\\mathbf{A}_{20}\mathbf{B}_{00}+\mathbf{A}_{21}\mathbf{B}_{10}\end{array}\right]\)</span>，此时 <span class="math inline">\(I=6,K=4,IB=2,KB=2,NIB=3,NKB=2\)</span>。</p>
<p>我们令 <span class="math inline">\(0\leq ib&lt;NIB\)</span> 和 <span class="math inline">\(0\leq kb&lt;NKB\)</span> 表示某个块的下标，如当 <span class="math inline">\(ib=1,\,kb=1\)</span> 时，则 <span class="math inline">\(\mathbf{A}_{ib,kb}=\left[\begin{array}{cc}a_{22} &amp; a_{23}\\a_{32} &amp; a_{33}\end{array}\right]\)</span>，<span class="math inline">\(\mathbf{B}_{kb,0}=\left[\begin{array}{c}b_{20}\\b_{30}\end{array}\right]\)</span>。</p>
<hr>
<h3 id="在mapreduce上实现分块算法">3 在MapReduce上实现分块算法</h3>
<p>我们只用一个MapRedcue阶段来实现算法。Mappper负责读取数据，并按照分块的策略发送分块，注意的是矩阵和向量分别存储于两个不同的输入路径且同时作为Mapper的输入数据。每个Reducers接收到Mapper发送的分块后，根据分块算法计算对应分块乘积的结果。例如，Reducer接收到块 <span class="math inline">\(\mathbf{A}_{ib,kb}\)</span> 和 <span class="math inline">\(\mathbf{B}_{kb,0}\)</span>，其中 <span class="math inline">\(0\leq kb&lt; NKB\)</span>，并按以下顺序重新组织：<span class="math display">\[\begin{align}\mathbf{A}_{ib,0}\,\,\mathbf{B}_{0,0}\,\,\mathbf{A}_{ib,1}\,\,\mathbf{B}_{1,0}\,\,\cdots \mathbf{A}_{ib,NKB-1}\,\,\mathbf{B}_{NKB-1,0}\end{align}\]</span> Reducer将每两个块 <span class="math inline">\(\mathbf{A}\)</span> 和 <span class="math inline">\(\mathbf{B}\)</span> 相乘并累加结果，即 <span class="math inline">\(\sum\limits_{kb=0}^{KB-1}\mathbf{A}_{ib,kb}\mathbf{B}_{kb,0}\)</span>。</p>
<hr>
<h3 id="算法分析">4 算法分析</h3>
<p>考虑最糟糕的情形，若矩阵 <span class="math inline">\(\mathcal{A}\)</span> 是稠密的而且无0元素，Mapper需要发送 <span class="math inline">\(I\times K\)</span> 个中间对。同理对于向量 <span class="math inline">\(\mathcal{B}\)</span>，Mapper需要发送 <span class="math inline">\(NIB\times K\)</span> 个中间对。即在最坏的情况下，会有 <span class="math inline">\(K\times (I+NIB)\)</span> 个中间对在soft和shuffle阶段传输。</p>
<hr>
<h3 id="伪代码">5 伪代码</h3>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/21/58ab9b4b72b77.png">

</div>
<p>注意到Mapper发送的key是个复合key，它由3个元素组成，分别是 <span class="math inline">\((ib,\, kb,\, m)\)</span>，其中 <span class="math inline">\(ib\)</span> 和 <span class="math inline">\(kb\)</span> 分别表示块中元素的行、列下标，而 <span class="math inline">\(m=0\)</span> 表示数据来自矩阵 <span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(m=1\)</span> 来自向量 <span class="math inline">\(\mathcal{B}\)</span>。</p>
<p>在sort和shuffle阶段，复合key必须有序，如按升序则先排 <span class="math inline">\(ib\)</span>，然后 <span class="math inline">\(kb\)</span>，最后 <span class="math inline">\(m\)</span>。同时，Partitioner需按等式 <span class="math inline">\(r=ib\pmod R\)</span> 来将中间对分发到各个Reducer中（<span class="math inline">\(R\)</span> 是Reducer的个数）。以上排序和划分复合key保证了每个Reducer都依据公式（1）接收到对应的 <span class="math inline">\(\mathbf{A}\)</span> 块和 <span class="math inline">\(\mathbf{B}\)</span> 块的数据。</p>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/21/58ab9b4b741c9.png">

</div>
<p>注意在Reducer阶段，我们只需要为矩阵 <span class="math inline">\(\mathcal{A}\)</span> 和向量 <span class="math inline">\(\mathcal{C}\)</span> 开辟内存空间。</p>
<hr>
<h3 id="源码">6 源码</h3>
<p>相关代码可参考 <a href="https://github.com/francize/Matrix-Vector-Multiplication" class="uri" target="_blank" rel="external">https://github.com/francize/Matrix-Vector-Multiplication</a>。</p>
<hr>
<h3 id="参考">7 参考</h3>
<ol style="list-style-type: decimal">
<li><a href="http://www.norstad.org/matrix-multiply/" class="uri" target="_blank" rel="external">http://www.norstad.org/matrix-multiply/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Block_matrix" class="uri" target="_blank" rel="external">https://en.wikipedia.org/wiki/Block_matrix</a></li>
<li><a href="https://vangjee.wordpress.com/2012/03/30/implementing-rawcomparator-will-speed-up-your-hadoop-mapreduce-mr-jobs-2/" target="_blank" rel="external">关于实现rawcomparator</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题定义&quot;&gt;1 问题定义&lt;/h3&gt;
&lt;p&gt;假设有矩阵 &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{A}=(a_{i,k})\in\mathbb{R}^{I\times K}\)&lt;/span&gt;，其中 &lt;span class=&quot;math
    
    </summary>
    
      <category term="Distributed Computing" scheme="http://www.shesong.org/categories/Distributed-Computing/"/>
    
    
      <category term="MapReduce" scheme="http://www.shesong.org/tags/MapReduce/"/>
    
      <category term="Matrix Multiplication" scheme="http://www.shesong.org/tags/Matrix-Multiplication/"/>
    
  </entry>
  
  <entry>
    <title>使用MapReduce框架度量簇集差异性</title>
    <link href="http://www.shesong.org/2016/10/03/%E4%BD%BF%E7%94%A8MapReduce%E6%A1%86%E6%9E%B6%E5%BA%A6%E9%87%8F%E7%B0%87%E9%9B%86%E5%B7%AE%E5%BC%82%E6%80%A7/"/>
    <id>http://www.shesong.org/2016/10/03/使用MapReduce框架度量簇集差异性/</id>
    <published>2016-10-03T08:33:41.000Z</published>
    <updated>2017-02-28T02:46:36.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题定义">1 问题定义</h2>
<ul>
<li><strong>簇集</strong>：给定一个非空集合<span class="math inline">\(\mathbf{D}\)</span>，<span class="math inline">\(C_1,\,C_2,\cdots,C_k\)</span>为集合<span class="math inline">\(\mathbf{D}\)</span>的划分，即对于任意的<span class="math inline">\(1\leq i,j\leq k\)</span>有<span class="math inline">\(C_i\bigcap C_j=\emptyset\)</span>，且<span class="math inline">\(\bigcup_{i=1}^kC_i=\mathbf{D}\)</span>，则由这样一组<strong>子集</strong>组成的集合被定义为集合<span class="math inline">\(\mathbf{D}\)</span>的簇集<span class="math inline">\(\mathcal{C}\)</span>，<span class="math inline">\(\mathcal{C}=\{C_1,\,C_2,\cdots,C_k\}\)</span>。</li>
<li><strong>元素的差异数</strong>：对于非空集合<span class="math inline">\(\mathbf{D}\)</span>的两个簇集<span class="math inline">\(\mathcal{C}_p=\{C_{p_1},\,C_{p_2},\cdots,C_{p_k}\}\)</span>和<span class="math inline">\(\mathcal{C}_q=\{C_{q_1},\,C_{q_2},\cdots,C_{q_m}\}\)</span>，给定一个元素<span class="math inline">\(u\in\mathbf{D}\)</span>，若<span class="math inline">\(u\in C_{p_x}\)</span>和<span class="math inline">\(u\in C_{q_y}\)</span>（其中<span class="math inline">\(C_{p_x}\in \mathcal{C}_p,C_{q_y}\in \mathcal{C}_q\)</span>），对于任意的<span class="math inline">\(v\)</span>若满足<span class="math inline">\(v\in C_{p_x}\)</span>但<span class="math inline">\(v\notin C_{q_y}\)</span>，我们说集合<span class="math inline">\(C_{p_x}\)</span>和<span class="math inline">\(C_{q_y}\)</span>之间存在差异。我们定义对于给定的<span class="math inline">\(u\)</span>，统计上述元素<span class="math inline">\(v\)</span>出现的次数，该次数即为元素<span class="math inline">\(u\)</span>的差异数，记为<span class="math inline">\(\Delta(u)\)</span>，其计算公式如下：<span class="math display">\[\begin{align}\Delta(u)=|C_{p_x}-C_{q_y}|+|C_{q_y}-C_{p_x}|=|C_{p_x}|+|C_{q_y}|-2\times|C_{p_x}\bigcap C_{q_y}| \end{align}\]</span></li>
<li><strong>簇集的差异率</strong>：簇集的差异率（记为<span class="math inline">\(d(\mathcal{C}_p,\mathcal{C}_q)\)</span>）即为集合<span class="math inline">\(\mathbf{D}\)</span>中所有元素的差异数之和<span class="math display">\[\begin{align}d(\mathcal{C}_p,\mathcal{C}_q)=\frac{\sum\limits_{i=1}^n\Delta(u_i)/2}{\binom{n}{2}}=\frac{\sum\limits_{i=1}^n\Delta(u_i)}{n\times (n-1)}\end{align}\]</span></li>
</ul>
<hr>
<h2 id="算法描述">2 算法描述</h2>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/27/58b441645de64.png">

</div>
<p>该算法只有一个MapReduce阶段，其输入必须仅仅是一个文本文件，该文件包含了所有的数据集，其数据集格式如上图所示，<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>指代不同的簇集，每一行表示该簇集中每个集合包含的元素。</p>
<h3 id="mapper">2.1 Mapper</h3>
<p>Mapper阶段主要对簇集中的元素分组并分发各组元素到各个Reducer，这也意味着有且仅有一个Mapper。好的分组策略可以平衡各个Reducer的计算工作负载，同时尽可能减少数据传输。我们用<span class="math inline">\(M(\mathcal{C})\)</span>代表簇集<span class="math inline">\(\mathcal{C}\)</span>中的最大集合，<span class="math inline">\(g(u)\)</span>表示每个元素<span class="math inline">\(u\)</span>的组号。显然，为了利用MapReduce框架的特性，组号应至少作为复合key的一部分。</p>
<p>现在考虑一种极端的情形，如果集合<span class="math inline">\(M(\mathcal{C})\)</span>中的每一个元素都不在同一组，而不同的reduce函数只能处理一组，这就意味着这个最大的集合会分发到很多个Reducer中，亦即增加了数据传输。</p>
<p>为了避免这种情况，集合<span class="math inline">\(M(\mathcal{C})\)</span>中的元素应尽可能的被分在同一组我们设计了一个策略：令<span class="math inline">\(C_I\)</span>、<span class="math inline">\(C_D\)</span>和<span class="math inline">\(C_C\)</span>分别表示集合<span class="math inline">\(M(\mathcal{C}_p)\)</span>和<span class="math inline">\(M(\mathcal{C}_q)\)</span>的交集、对称差集和补集： <span class="math display">\[\begin{align}
C_I=M(\mathcal{C}_p)\bigcap M(\mathcal{C}_q)
\end{align}\]</span> <span class="math display">\[\begin{align}
C_D=M(\mathcal{C}_p)\bigcup M(\mathcal{C}_q)\setminus C_I
\end{align}\]</span> <span class="math display">\[\begin{align}
C_C=\mathbf{D}\setminus M(\mathcal{C}_p)\bigcup M(\mathcal{C}_q)
\end{align}\]</span> 首先我们令<span class="math inline">\(C_I\)</span>中的元素都在同一组，其组号为0。<span class="math inline">\(g_1\)</span>和<span class="math inline">\(g_2\)</span>分别表示<span class="math inline">\(C_D\)</span>和<span class="math inline">\(C_C\)</span>分组的个数，<span class="math inline">\(\phi(u_i)\)</span>表示集合中各个元素的下标。完整的分组策略如下： <span class="math display">\[\begin{align}
g(v_i) = 
\begin{cases}
    0 &amp; \text{if} \  u_i \in C_I \\
    H_1(u_i) = 1 + \phi(u_i) \bmod g_1 &amp; \text{if}\  u_i \in C_D\\
    H_2(u_i) = 1 + g_1 + \phi(u_i) \bmod g_2 &amp; \text{if} \  u_i \in C_C
\end{cases}
\end{align}\]</span></p>
<p>我们上面说了组号应至少作为复合key的一部分，那另一部分是什么呢？我们为了计算差异数，至少要判别元素原先是属于哪个簇集，也就意味着复合key的另一部分必须是簇集的标志。更进一步，集合中的元素被分成了多少组，则该集合就应该被分发多少次。同时，Mapper输出的value是有形式如<span class="math inline">\([u_i:g(u_i)]\)</span>（元素：元素的组号）组成的元组。</p>
<p>如上图所示，集合<span class="math inline">\(\mathbf{D}=\{1,2,3,4,5,6,7,8,9\}\)</span>，依据上述分组策略，我们有<span class="math inline">\(M(\mathcal{C}_p)=\{1,2,3,4,6\}\)</span>，<span class="math inline">\(M(\mathcal{C}_q)=\{1,2,3,4,8\}\)</span>，元素<span class="math inline">\((1,2,3,4)\)</span>被分在组0，元素<span class="math inline">\((6,8)\)</span>被分在组1，元素<span class="math inline">\((5,7,9)\)</span>被分在组2。由于集合<span class="math inline">\(\{1,2,3,4,6\}\)</span>中元素<span class="math inline">\((1,2,3,4)\)</span>在组0而元素<span class="math inline">\((6)\)</span>在组1，所以该集合会被分发两次。</p>
<h3 id="shuffle-and-sort">2.2 Shuffle and Sort</h3>
<p>由上一节可知Mapper的输出为一系列元组形式如$(g(u_j):p|q), [u_i:g(u_i)]^+<span class="math inline">\(，这些元组首先按复合key的\)</span>g(u_j)<span class="math inline">\(其次再是簇集的标志\)</span>p<span class="math inline">\(或\)</span>q$来排升序。其次重定义Paritioner根据公式 <span class="math inline">\(r=g(u_j)\pmod R\)</span>来将元组划分到不同的Reducer中。最后在GroupComparator中只比较复合key的<span class="math inline">\(g(u_j)\)</span>而忽略比较簇集的标志，以保证每一个Reducer接收同样的key。</p>
<h3 id="reducer">2.3 Reducer</h3>
<p>在Redcuer中，由于每个value中各个元素后都附带着该元素所在的组号，而当前Reducer对应的复合key只包含一个组号<span class="math inline">\(g\)</span>，所以我们只需要遍历一次所有的value（sort阶段后有序）并仅需计算组号是<span class="math inline">\(g\)</span>的元素的差异数即可。</p>
<hr>
<h2 id="伪代码和源码">3 伪代码和源码</h2>
<h3 id="伪代码">3.1 伪代码</h3>
<div class="figure">
<img src="https://ooo.0o0.ooo/2017/02/28/58b4dc817e317.png">

</div>
<p>注意我们在自定义了一个全局Counter（org.apache.hadoop.mapreduce.Counter in Hadoop API）来统计各个Reducer中计算得到的差异数，所以Reducer可以不必输出。</p>
<h3 id="源码">3.2 源码</h3>
<p>相关代码可参考<a href="https://github.com/francize/FastMeasureClusterings" class="uri" target="_blank" rel="external">https://github.com/francize/FastMeasureClusterings</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题定义&quot;&gt;1 问题定义&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;簇集&lt;/strong&gt;：给定一个非空集合&lt;span class=&quot;math inline&quot;&gt;\(\mathbf{D}\)&lt;/span&gt;，&lt;span class=&quot;math inline&quot;&gt;\
    
    </summary>
    
      <category term="Distributed Computing" scheme="http://www.shesong.org/categories/Distributed-Computing/"/>
    
    
      <category term="MapReduce" scheme="http://www.shesong.org/tags/MapReduce/"/>
    
      <category term="簇集" scheme="http://www.shesong.org/tags/%E7%B0%87%E9%9B%86/"/>
    
  </entry>
  
</feed>
